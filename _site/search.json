[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guan Gui",
    "section": "",
    "text": "Hello! I am Guan Gui, a data science and biostatistics student with a passion for uncovering insights from complex biomedical data. I am currently pursuing my ScM in Biostatistics at Johns Hopkins Bloomberg School of Public Health. I enjoy exploring new data science methods and their applications in public health."
  },
  {
    "objectID": "index.html#course",
    "href": "index.html#course",
    "title": "Home",
    "section": "",
    "text": "Background\nPart 1\nPart 2\nPart 3"
  },
  {
    "objectID": "index.html#author",
    "href": "index.html#author",
    "title": "Home",
    "section": "",
    "text": "**Guan Gui*"
  },
  {
    "objectID": "index.html#published",
    "href": "index.html#published",
    "title": "Home",
    "section": "",
    "text": "October 24, 2024"
  },
  {
    "objectID": "part1.html",
    "href": "part1.html",
    "title": "Part 1: Defining My Scientific Brand",
    "section": "",
    "text": "I am Guan Gui, a first-class honors graduate in Data Science from the University of Sydney, currently pursuing my ScM in Biostatistics at Johns Hopkins University. I have a keen sensitivity to data and extensive experience in data analysis, with a strong passion for biomedical data, especially in the field of spatial genomics.\n\n\n\nI am dedicated to uncovering new insights from biomedical data, particularly in cancer and neurodegenerative disease research. By developing R packages, my aim is to create innovative tools and publish research that offer fresh perspectives on data, enabling new discoveries and making a meaningful impact on the medical and scientific community.\n\n\n\nBased on my experience, my primary audience consists of fellow researchers working in similar fields. I develop open-source R packages that extract new features from spatial genomics data, aiming to provide unique insights for this type of data. As open-source software, my work is accessible for others to use, improve, or benchmark against, fostering collaboration and progress within the field.\n\n\n\nMy motivation stems from my family, who have been affected by cancer and neurodegenerative diseases, inspiring me to contribute to advancements in disease research. My SpatialFeatures package has been submitted to Bioconductor and received a high distinction grade as my honors project. I can spend countless hours on data analysis projects that intrigue me, thoroughly enjoying the process of discovering new insights step by step. In the next five to ten years, I hope to pursue a PhD with meaningful contributions, graduate successfully, and secure a position in biomedical data science that allows me to continue working on impactful projects."
  },
  {
    "objectID": "part1.html#background",
    "href": "part1.html#background",
    "title": "Part 1",
    "section": "",
    "text": "Read this blogpost titled Building a brand as a scientist. Reflect on the questions in the ‚ÄúDefining your brand‚Äù section.\nWrite two paragraphs (4-6 sentences) max here answering one (or more) of the questions asked in the section."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently an ScM student in Biostatistics at Johns Hopkins Bloomberg School of Public Health, following my completion of a first-class honours degree in Data Science at the University of Sydney. My research interests span biostatistics, spatial genomics, and machine learning, with a focus on analyzing and deriving insights from complex data.\n\n\n\nMaster of Science in Biostatistics (ScM) Sep.¬†2024 - Present\nüìç Baltimore, MD\nJohns Hopkins Bloomberg School of Public Health\nBachelor of Science (Honours) in Data Science Mar.¬†2021 - Jul.¬†2024\nüìç Sydney, Australia\nUniversity of Sydney\nGraduated with First Class Honours\nAwards: Vice Chancellor‚Äôs Global Mobility Scholarship, Dalyell Scholar, Charles Perkins Centre Summer Research Scholarship\nSemester Exchange Aug.¬†2022 - Dec.¬†2022\nüìç Singapore\nNational University of Singapore\nSummer & Winter Schools Dec.¬†2021, Jul.¬†2022\nüìç Shanghai, China\nShanghai Jiao Tong University\n\n\n\n\n\nSpatialFeatures Project Jul.¬†2023 - Present\nüìç Sydney, Australia\nUniversity of Sydney\nDeveloped the ‚ÄúSpatialFeatures‚Äù algorithm to analyze spatial genomics data at cellular and sub-cellular levels, creating a versatile R package for use across genomics datasets.\nData Analyst Intern Jan.¬†- May 2022\nüìç Shanghai, China\nDiAct Technology Co., Ltd.¬†(Ipsos)\nAnalyzed social media data for automotive trends, improving data processing by 30% through Python optimizations.\nUser Growth Intern Jul.¬†- Oct.¬†2021\nüìç Beijing, China\nJD.com\nEnhanced SQL processes and developed metrics for improved user engagement recommendations.\n\n\n\n\n\nProgramming: R, Python, SQL, SAS, Java, Tableau\nLanguages: Chinese (Native), English (Fluent)\n\n\n\n\nOutside of academics, I love fitness and playing Apex Legends."
  },
  {
    "objectID": "about.html#who-i-am",
    "href": "about.html#who-i-am",
    "title": "About",
    "section": "",
    "text": "I am a student at Johns Hopkins, studying biostatistics and honing my skills in data science and statistical programming. My background is rooted in data analysis, with a strong interest in statistical methodologies and command-line workflows."
  },
  {
    "objectID": "about.html#professional-interests",
    "href": "about.html#professional-interests",
    "title": "About",
    "section": "",
    "text": "My current focus is on statistical programming, command-line tools, and the development of websites for data presentation and analysis."
  },
  {
    "objectID": "about.html#hobbies-and-interests",
    "href": "about.html#hobbies-and-interests",
    "title": "About",
    "section": "",
    "text": "Outside of academics, I enjoy [mention a hobby or interest here, optionally including a related photo if you‚Äôre comfortable sharing]."
  },
  {
    "objectID": "example_analysis.html",
    "href": "example_analysis.html",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "",
    "text": "This analysis investigates the Breast Cancer Wisconsin (Diagnostic) dataset to answer the question: How effectively can the Random Forest method predict breast cancer diagnosis, and which variables are most crucial in distinguishing between malignant and benign tumors?\nThe intended audience includes medical researchers and clinicians focused on diagnostic features that support early detection and classification of breast cancer.\nThe dataset, originally collected by Dr.¬†William H. Wolberg, is available through the UCI Machine Learning Repository (Wolberg and Mangasarian 1993). The repository hosts detailed tumor measurements essential for assessing feature importance in classification tasks.\nBelow is an image from the Kaggle Breast Cancer Wisconsin (Diagnostic) Data Set website, representing breast cancer cells (Repository 2023).\n\n\n\nBreast Cancer Cells\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis analysis highlights diagnostic features in breast cancer, offering foundational insights that may support the development of advanced machine learning models.\n\n\n\n\n\nThe data dictionary below, detailing each variable‚Äôs description and relevance to the analysis, particularly in identifying differences between benign and malignant tumor characteristics. The original data dictionary also can be found at UCI Machine Learning Repository.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nUnique identifier for each patient\n\n\nDiagnosis\nDiagnosis of the tumor (M = malignant, B = benign)\n\n\nradius_mean\nMean radius: mean of distances from center to points on the perimeter\n\n\ntexture_mean\nMean texture: standard deviation of gray-scale values\n\n\nperimeter_mean\nMean perimeter: mean size of the core tumor perimeter\n\n\narea_mean\nMean area: mean size of the core tumor area\n\n\nsmoothness_mean\nMean smoothness: local variation in radius lengths\n\n\ncompactness_mean\nMean compactness: calculated as (perimeter^2 / area - 1.0)\n\n\nconcavity_mean\nMean concavity: severity of concave portions of the contour\n\n\nconcave.points_mean\nMean concave points: number of concave portions of the contour\n\n\nsymmetry_mean\nMean symmetry: measure of symmetry of cell nucleus\n\n\nfractal_dimension_mean\nMean fractal dimension: ‚Äúcoastline approximation‚Äù - 1\n\n\nradius_se\nStandard error of radius\n\n\ntexture_se\nStandard error of texture\n\n\nperimeter_se\nStandard error of perimeter\n\n\narea_se\nStandard error of area\n\n\nsmoothness_se\nStandard error of smoothness\n\n\ncompactness_se\nStandard error of compactness\n\n\nconcavity_se\nStandard error of concavity\n\n\nconcave.points_se\nStandard error of concave points\n\n\nsymmetry_se\nStandard error of symmetry\n\n\nfractal_dimension_se\nStandard error of fractal dimension\n\n\nradius_worst\nWorst or largest value of radius (mean of the three largest values)\n\n\ntexture_worst\nWorst or largest value of texture\n\n\nperimeter_worst\nWorst or largest value of perimeter\n\n\narea_worst\nWorst or largest value of area\n\n\nsmoothness_worst\nWorst or largest value of smoothness\n\n\ncompactness_worst\nWorst or largest value of compactness\n\n\nconcavity_worst\nWorst or largest value of concavity\n\n\nconcave.points_worst\nWorst or largest value of concave points\n\n\nsymmetry_worst\nWorst or largest value of symmetry\n\n\nfractal_dimension_worst\nWorst or largest value of fractal dimension"
  },
  {
    "objectID": "example_analysis.html#introduction",
    "href": "example_analysis.html#introduction",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "",
    "text": "This analysis investigates the Breast Cancer Wisconsin (Diagnostic) dataset to evaluate the Random Forest method‚Äôs performance in predicting breast cancer diagnosis and identify key variables that distinguish between malignant and benign tumors.\nThe intended audience includes medical researchers and clinicians focused on diagnostic features that support early detection and classification of breast cancer.\nThe dataset, originally collected by Dr.¬†William H. Wolberg, is available through the UCI Machine Learning Repository (Wolberg and Mangasarian 1993). The repository hosts detailed tumor measurements essential for assessing feature importance in classification tasks.\nBelow is an image from the Kaggle Breast Cancer Wisconsin (Diagnostic) Data Set, representing breast cancer cells (Repository 2023).\n\n\n\nBreast Cancer Cells\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis analysis provides insights into diagnostic features in breast cancer, potentially guiding more advanced machine learning models."
  },
  {
    "objectID": "example_analysis.html#objective",
    "href": "example_analysis.html#objective",
    "title": "Example Analysis",
    "section": "",
    "text": "The primary question I aim to answer is [insert research question here]. This analysis is aimed at an audience interested in [describe audience]."
  },
  {
    "objectID": "example_analysis.html#data-source",
    "href": "example_analysis.html#data-source",
    "title": "Example Analysis",
    "section": "",
    "text": "The data comes from [link to dataset source]. A data dictionary is available at [link to data dictionary or include one below]."
  },
  {
    "objectID": "example_analysis.html#data-wrangling",
    "href": "example_analysis.html#data-wrangling",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "3 Data Wrangling",
    "text": "3 Data Wrangling\nThe data wrangling steps refine the Breast Cancer dataset for analysis.\n\n\n\n\n\n\nTip\n\n\n\nCorrelations help identify features highly associated with tumor diagnosis, aiding in feature selection for the model.\n\n\nSteps:\n\nRemoved unnecessary columns (id and X) to focus on diagnostic features.\nRenamed the diagnosis column to Diagnosis for clarity.\nDropped rows with missing values to ensure complete cases.\nConverted the Diagnosis column into a factor with levels ‚ÄúB‚Äù (benign) and ‚ÄúM‚Äù (malignant).\nCalculated correlations with the diagnosis to identify highly predictive variables, retaining only those with correlation &gt; |0.3|.\n\n\n3.1 Functions Used\n\nselect(): Excludes specific columns and selects highly correlated variables.\nrename(): Renames diagnosis to Diagnosis for consistency.\ndrop_na(): Removes rows with missing values.\nmutate(): Creates or transforms columns, such as converting Diagnosis to a factor and creating a numeric version for correlation.\nsummarize() + across(): Calculates correlation values for each feature with Diagnosis.\npivot_longer(): Reshapes the data to make correlation results easier to filter.\narrange(): Sorts correlations by their absolute values.\nfilter(): Selects only variables with a high correlation to the target variable.\n\n\n\nShow code\n# Load data\ndata &lt;- read.csv(\"example_analysis_data.csv\")\n\n# Data wrangling with dplyr and tidyr\ndata &lt;- data %&gt;%\n  select(-id, -X) %&gt;%              \n  rename(Diagnosis = diagnosis) %&gt;% \n  drop_na() %&gt;%                     \n  mutate(Diagnosis = factor(Diagnosis, levels = c(\"B\", \"M\"))) \n\n# Calculate correlations and arrange by correlation with target variable (Diagnosis)\ndata_numeric &lt;- data %&gt;%\n  mutate(Diagnosis_num = as.numeric(Diagnosis) - 1) # Converts factor levels to 0 (B) and 1 (M)\n\ncorrelations &lt;- data_numeric %&gt;%\n  select(-Diagnosis) %&gt;% \n  summarize(across(-Diagnosis_num, ~ cor(., data_numeric$Diagnosis_num, use = \"complete.obs\"))) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"correlation\") %&gt;%\n  arrange(desc(abs(correlation)))\n\n# Print top 10 correlation variables\ntop_10_correlations &lt;- correlations %&gt;% head(10)\nprint(top_10_correlations)\n\n\n# A tibble: 10 √ó 2\n   variable             correlation\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 concave.points_worst       0.794\n 2 perimeter_worst            0.783\n 3 concave.points_mean        0.777\n 4 radius_worst               0.776\n 5 perimeter_mean             0.743\n 6 area_worst                 0.734\n 7 radius_mean                0.730\n 8 area_mean                  0.709\n 9 concavity_mean             0.696\n10 concavity_worst            0.660\n\n\nShow code\ncor_threshold &lt;- 0.3\nhigh_corr_vars &lt;- correlations %&gt;%\n  filter(abs(correlation) &gt;= cor_threshold) %&gt;%\n  pull(variable)\n\n# Keep only highly correlated variables in the original data\ndata &lt;- data %&gt;%\n  select(all_of(high_corr_vars), Diagnosis)\n\nhead(data)"
  },
  {
    "objectID": "example_analysis.html#analysis-and-visualization",
    "href": "example_analysis.html#analysis-and-visualization",
    "title": "Example Analysis",
    "section": "",
    "text": "Below are some visualizations created with ggplot2: 1. Plot 1 - [Describe plot] 2. Plot 2 - [Describe plot] 3. Plot 3 - [Describe plot with faceting]\n![Example image or table here]\n\nNote: This analysis references three sources, which can be found in the accompanying .bib file."
  },
  {
    "objectID": "example_analysis.html#summary",
    "href": "example_analysis.html#summary",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "4 Summary",
    "text": "4 Summary\nThe analysis of the Breast Cancer Wisconsin (Diagnostic) dataset revealed that features such as concave.points_worst, perimeter_worst, and concave.points_mean exhibit strong correlations with tumor diagnosis. Dimensionality reduction through PCA indicated that a few principal components account for the majority of variance, efficiently reducing feature redundancy. A Random Forest model, trained and evaluated on the dataset, achieved high accuracy and AUC, demonstrating its effectiveness in tumor classification. These findings provide essential insights into diagnostic markers, supporting potential advancements in early cancer detection and classification models."
  },
  {
    "objectID": "part3.html",
    "href": "part3.html",
    "title": "Part 3: Command Line Operations on students.csv",
    "section": "",
    "text": "To download the students.csv file from the provided GitHub URL, use the wget command:\nwhich wget\nrm -f students.csv\n/opt/homebrew/bin/wget https://raw.githubusercontent.com/stephaniehicks/jhustatprogramming2024/main/projects/01-project/students.csv\n\n\n\ncat students.csv\n\n\n\nhead -n 5 students.csv\n\n\n\ntail -n 3 students.csv\n\n\n\nwc -l students.csv\n\n\n\ngrep \"Math\" students.csv\n\n\n\nUsing grep:\ngrep \",F,\" students.csv\n\n\n\nsort -t \",\" -k3,3n students.csv\n\n\n\ncut -d \",\" -f6 students.csv | sort | uniq\n\n\n\nawk -F \",\" '{sum+=$5} END {print \"Average Grade:\", sum/(NR-1)}' students.csv\n\n\n\nTo make an in-place replacement:\nsed -i '' 's/Math/Mathematics/g' students.csv\n\n\n\ncat students.csv"
  },
  {
    "objectID": "part3.html#command-line-exercise",
    "href": "part3.html#command-line-exercise",
    "title": "Part 3",
    "section": "",
    "text": "This section demonstrates basic command-line skills using a sample dataset students.csv.\n\n\nTo download the dataset, use the following command: ```bash wget https://example.com/students.csv"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Guan Gui",
    "section": "About Me",
    "text": "About Me\nI am a dedicated researcher and data enthusiast, particularly interested in biostatistics, spatial genomics, and machine learning. My goal is to contribute to innovative solutions in biomedical research by applying advanced statistical and computational techniques.\n\nPublished on October 24, 2024"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Home",
    "section": "",
    "text": "Master of Science in Biostatistics (ScM)\nJohns Hopkins Bloomberg School of Public Health\n2024 ‚Äì Present\nBachelor of Science (Honours) in Data Science\nUniversity of Sydney\n2021 ‚Äì 2024\nGraduated with First Class Honours\nAwards: Vice Chancellor‚Äôs Global Mobility Scholarship, Dalyell Scholar, Charles Perkins Centre Summer Research Scholarship\nSemester Exchange\nNational University of Singapore\nAug 2022 ‚Äì Dec 2022\nSummer & Winter Schools\nShanghai Jiao Tong University\n2022 - 2023"
  },
  {
    "objectID": "index.html#research-and-projects",
    "href": "index.html#research-and-projects",
    "title": "Home",
    "section": "",
    "text": "Developed the ‚ÄúSpatialFeatures‚Äù algorithm, capturing super-cellular and sub-cellular patterns in spatial genomics data. Created an R package for wide applications in genomics research.\n\n\n\nApplied Vision Transformer (ViT) deep learning models to classify frozen mouse brain cells, achieving high accuracy. This project included interactive features for users through a Shiny app.\n\n\n\nSimulated disease spread using network models with the SIR framework, providing insights into the dynamics of infection spread.\n\n\n\nDeveloped and validated machine learning models for breast cancer diagnosis, achieving high predictive accuracy through LDA and Random Forest."
  },
  {
    "objectID": "index.html#professional-experience",
    "href": "index.html#professional-experience",
    "title": "Home",
    "section": "",
    "text": "Data Analyst Intern\nDiAct Technology Co., Ltd.¬†(Ipsos), Shanghai, China\nJan 2022 ‚Äì May 2022\nImproved data processing by 30% through Python optimization, and provided data insights for automotive brands like SAIC Volkswagen and Audi.\nUser Growth Intern\nJD.com, Beijing, China\nJul 2021 ‚Äì Sep 2021\nEnhanced SQL procedures and developed personalized user experience metrics, improving recommendation accuracy and efficiency."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Home",
    "section": "",
    "text": "Programming: R, Python, SQL, SAS, Java\nTools: Quarto, Jupyter, Git, Tableau\nMachine Learning: TensorFlow, scikit-learn, ggplot2, dplyr, tidyr\nLanguages: Chinese (Native), English (Fluent)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Home",
    "section": "",
    "text": "Feel free to reach out at ggui2@jh.edu.\n\nPublished on October 24, 2024"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Master of Science in Biostatistics (ScM) Sep.¬†2024 - Present\nüìç Baltimore, MD\nJohns Hopkins Bloomberg School of Public Health\nBachelor of Science (Honours) in Data Science Mar.¬†2021 - Jul.¬†2024\nüìç Sydney, Australia\nUniversity of Sydney\nGraduated with First Class Honours\nAwards: Vice Chancellor‚Äôs Global Mobility Scholarship, Dalyell Scholar, Charles Perkins Centre Summer Research Scholarship\nSemester Exchange Aug.¬†2022 - Dec.¬†2022\nüìç Singapore\nNational University of Singapore\nSummer & Winter Schools Dec.¬†2021, Jul.¬†2022\nüìç Shanghai, China\nShanghai Jiao Tong University"
  },
  {
    "objectID": "about.html#research-and-professional-experience",
    "href": "about.html#research-and-professional-experience",
    "title": "About",
    "section": "",
    "text": "SpatialFeatures Project Jul.¬†2023 - Present\nüìç Sydney, Australia\nUniversity of Sydney\nDeveloped the ‚ÄúSpatialFeatures‚Äù algorithm to analyze spatial genomics data at cellular and sub-cellular levels, creating a versatile R package for use across genomics datasets.\nData Analyst Intern Jan.¬†- May 2022\nüìç Shanghai, China\nDiAct Technology Co., Ltd.¬†(Ipsos)\nAnalyzed social media data for automotive trends, improving data processing by 30% through Python optimizations.\nUser Growth Intern Jul.¬†- Oct.¬†2021\nüìç Beijing, China\nJD.com\nEnhanced SQL processes and developed metrics for improved user engagement recommendations."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "",
    "text": "Programming: R, Python, SQL, SAS, Java, Tableau\nLanguages: Chinese (Native), English (Fluent)"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About",
    "section": "",
    "text": "Outside of academics, I love fitness and playing Apex Legends."
  },
  {
    "objectID": "part1.html#who-am-i-what-makes-me-special",
    "href": "part1.html#who-am-i-what-makes-me-special",
    "title": "Part 1: Defining My Scientific Brand",
    "section": "",
    "text": "I am Guan Gui, a first-class honors graduate in Data Science from the University of Sydney, currently pursuing my ScM in Biostatistics at Johns Hopkins University. I have a keen sensitivity to data and extensive experience in data analysis, with a strong passion for biomedical data, especially in the field of spatial genomics."
  },
  {
    "objectID": "part1.html#what-do-i-stand-for",
    "href": "part1.html#what-do-i-stand-for",
    "title": "Part 1: Defining My Scientific Brand",
    "section": "",
    "text": "I am dedicated to uncovering new insights from biomedical data, particularly in cancer and neurodegenerative disease research. By developing R packages, my aim is to create innovative tools and publish research that offer fresh perspectives on data, enabling new discoveries and making a meaningful impact on the medical and scientific community."
  },
  {
    "objectID": "part1.html#who-is-my-audience",
    "href": "part1.html#who-is-my-audience",
    "title": "Part 1: Defining My Scientific Brand",
    "section": "",
    "text": "Based on my experience, my primary audience consists of fellow researchers working in similar fields. I develop open-source R packages that extract new features from spatial genomics data, aiming to provide unique insights for this type of data. As open-source software, my work is accessible for others to use, improve, or benchmark against, fostering collaboration and progress within the field."
  },
  {
    "objectID": "part1.html#what-is-my-goal",
    "href": "part1.html#what-is-my-goal",
    "title": "Part 1: Defining My Scientific Brand",
    "section": "",
    "text": "My motivation stems from my family, who have been affected by cancer and neurodegenerative diseases, inspiring me to contribute to advancements in disease research. My SpatialFeatures package has been submitted to Bioconductor and received a high distinction grade as my honors project. I can spend countless hours on data analysis projects that intrigue me, thoroughly enjoying the process of discovering new insights step by step. In the next five to ten years, I hope to pursue a PhD with meaningful contributions, graduate successfully, and secure a position in biomedical data science that allows me to continue working on impactful projects."
  },
  {
    "objectID": "part3.html#command-line-solutions",
    "href": "part3.html#command-line-solutions",
    "title": "Part 3: Command Line Operations on students.csv",
    "section": "",
    "text": "To download the students.csv file from the provided GitHub URL, use the wget command:\nwhich wget\nrm -f students.csv\n/opt/homebrew/bin/wget https://raw.githubusercontent.com/stephaniehicks/jhustatprogramming2024/main/projects/01-project/students.csv\n\n\n\ncat students.csv\n\n\n\nhead -n 5 students.csv\n\n\n\ntail -n 3 students.csv\n\n\n\nwc -l students.csv\n\n\n\ngrep \"Math\" students.csv\n\n\n\nUsing grep:\ngrep \",F,\" students.csv\n\n\n\nsort -t \",\" -k3,3n students.csv\n\n\n\ncut -d \",\" -f6 students.csv | sort | uniq\n\n\n\nawk -F \",\" '{sum+=$5} END {print \"Average Grade:\", sum/(NR-1)}' students.csv\n\n\n\nTo make an in-place replacement:\nsed -i '' 's/Math/Mathematics/g' students.csv\n\n\n\ncat students.csv"
  },
  {
    "objectID": "example_analysis.html#data-dictionary",
    "href": "example_analysis.html#data-dictionary",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "2 Data Dictionary",
    "text": "2 Data Dictionary\nThe data dictionary below, detailing each variable‚Äôs description and relevance to the analysis, particularly in identifying differences between benign and malignant tumor characteristics.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nUnique identifier for each patient\n\n\nDiagnosis\nDiagnosis of the tumor (M = malignant, B = benign)\n\n\nradius_mean\nMean radius: mean of distances from center to points on the perimeter\n\n\ntexture_mean\nMean texture: standard deviation of gray-scale values\n\n\nperimeter_mean\nMean perimeter: mean size of the core tumor perimeter\n\n\narea_mean\nMean area: mean size of the core tumor area\n\n\nsmoothness_mean\nMean smoothness: local variation in radius lengths\n\n\ncompactness_mean\nMean compactness: calculated as (perimeter^2 / area - 1.0)\n\n\nconcavity_mean\nMean concavity: severity of concave portions of the contour\n\n\nconcave.points_mean\nMean concave points: number of concave portions of the contour\n\n\nsymmetry_mean\nMean symmetry: measure of symmetry of cell nucleus\n\n\nfractal_dimension_mean\nMean fractal dimension: ‚Äúcoastline approximation‚Äù - 1\n\n\nradius_se\nStandard error of radius\n\n\ntexture_se\nStandard error of texture\n\n\nperimeter_se\nStandard error of perimeter\n\n\narea_se\nStandard error of area\n\n\nsmoothness_se\nStandard error of smoothness\n\n\ncompactness_se\nStandard error of compactness\n\n\nconcavity_se\nStandard error of concavity\n\n\nconcave.points_se\nStandard error of concave points\n\n\nsymmetry_se\nStandard error of symmetry\n\n\nfractal_dimension_se\nStandard error of fractal dimension\n\n\nradius_worst\nWorst or largest value of radius (mean of the three largest values)\n\n\ntexture_worst\nWorst or largest value of texture\n\n\nperimeter_worst\nWorst or largest value of perimeter\n\n\narea_worst\nWorst or largest value of area\n\n\nsmoothness_worst\nWorst or largest value of smoothness\n\n\ncompactness_worst\nWorst or largest value of compactness\n\n\nconcavity_worst\nWorst or largest value of concavity\n\n\nconcave.points_worst\nWorst or largest value of concave points\n\n\nsymmetry_worst\nWorst or largest value of symmetry\n\n\nfractal_dimension_worst\nWorst or largest value of fractal dimension"
  },
  {
    "objectID": "example_analysis.html#exploratory-data-analysis",
    "href": "example_analysis.html#exploratory-data-analysis",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "2 Exploratory Data Analysis",
    "text": "2 Exploratory Data Analysis\n\n2.1 Data Wrangling\nThe data wrangling steps refine the Breast Cancer dataset for analysis.\n\n\n\n\n\n\nTip\n\n\n\nCorrelations help identify features highly associated with tumor diagnosis, aiding in feature selection for the model.\n\n\nSteps:\n\nRemoved unnecessary columns (id and X) to focus on diagnostic features.\nRenamed the diagnosis column to Diagnosis for clarity.\nDropped rows with missing values to ensure complete cases.\nConverted the Diagnosis column into a factor with levels ‚ÄúB‚Äù (benign) and ‚ÄúM‚Äù (malignant).\nCalculated correlations with the diagnosis to identify highly predictive variables, retaining only those with correlation &gt; |0.3|.\n\nFunctions Used:\n\nselect(): Excludes specific columns and selects highly correlated variables.\nrename(): Renames diagnosis to Diagnosis for consistency.\ndrop_na(): Removes rows with missing values.\nmutate(): Creates or transforms columns, such as converting Diagnosis to a factor and creating a numeric version for correlation.\nsummarize() + across()**: Calculates correlation values for each feature with Diagnosis.\npivot_longer(): Reshapes the data to make correlation results easier to filter.\narrange(): Sorts correlations by their absolute values.\nfilter(): Selects only variables with a high correlation to the target variable.\n\n\n# Load data\ndata &lt;- read.csv(\"example_analysis_data.csv\")\n\n# Data wrangling with dplyr and tidyr\ndata &lt;- data %&gt;%\n  select(-id, -X) %&gt;%              \n  rename(Diagnosis = diagnosis) %&gt;% \n  drop_na() %&gt;%                     \n  mutate(Diagnosis = factor(Diagnosis, levels = c(\"B\", \"M\"))) \n\n# Calculate correlations and arrange by correlation with target variable (Diagnosis)\ndata_numeric &lt;- data %&gt;%\n  mutate(Diagnosis_num = as.numeric(Diagnosis) - 1) # Converts factor levels to 0 (B) and 1 (M)\n\ncorrelations &lt;- data_numeric %&gt;%\n  select(-Diagnosis) %&gt;% \n  summarize(across(-Diagnosis_num, ~ cor(., data_numeric$Diagnosis_num, use = \"complete.obs\"))) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"correlation\") %&gt;%\n  arrange(desc(abs(correlation)))\n\n# Print top 10 correlation variables\ntop_10_correlations &lt;- correlations %&gt;% head(10)\ncat(\"Top 10 Variables Most Correlated with Diagnosis:\\n\")\n\nTop 10 Variables Most Correlated with Diagnosis:\n\nprint(top_10_correlations)\n\n# A tibble: 10 √ó 2\n   variable             correlation\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 concave.points_worst       0.794\n 2 perimeter_worst            0.783\n 3 concave.points_mean        0.777\n 4 radius_worst               0.776\n 5 perimeter_mean             0.743\n 6 area_worst                 0.734\n 7 radius_mean                0.730\n 8 area_mean                  0.709\n 9 concavity_mean             0.696\n10 concavity_worst            0.660\n\ncor_threshold &lt;- 0.3\nhigh_corr_vars &lt;- correlations %&gt;%\n  filter(abs(correlation) &gt;= cor_threshold) %&gt;%\n  pull(variable)\n\n# Keep only highly correlated variables in the original data\ndata &lt;- data %&gt;%\n  select(all_of(high_corr_vars), Diagnosis)\n\ncat(\"Dimensions of the Refined Dataset:\\n\")\n\nDimensions of the Refined Dataset:\n\ndim(data)\n\n[1] 569  24\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe refined dataset contains 569 rows and 24 columns, focusing on variables with significant correlations to the diagnosis. The ten variables most strongly correlated with breast cancer diagnosis are led by concave.points_worst (0.79) and perimeter_worst (0.78).\n\n\n\n\n2.2 Target Variable Distribution\nThis bar plot shows the distribution of benign and malignant diagnoses in the dataset.\n\n# Plot the distribution of tumor diagnoses\nggplot(data, aes(x = Diagnosis, fill = Diagnosis)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.3) +\n  labs(\n    title = \"Distribution of Tumor Diagnoses in the Dataset\",\n    subtitle = \"Benign tumors are more frequent than malignant tumors\",\n    caption = \"This plot shows the count of benign and malignant tumors in the dataset\",\n    x = \"Tumor Diagnosis\",\n    y = \"Count of Cases\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nBenign tumors are more common than malignant tumors, providing a slightly imbalanced but sufficient dataset for training.\n\n\n2.3 Feature Distribution by Diagnosis\nThis box plot compares the distribution of concave.points_mean and perimeter_mean between benign and malignant tumors, both of which are highly correlated with the target variable.\n\n# Compare the distribution of 'concave.points_mean' and 'perimeter_mean' across diagnoses using a box plot\nggplot(data %&gt;% select(Diagnosis, concave.points_mean, perimeter_mean) %&gt;%\n         pivot_longer(cols = -Diagnosis, names_to = \"Feature\", values_to = \"Value\"), \n       aes(x = Diagnosis, y = Value, fill = Diagnosis)) +\n  geom_boxplot(alpha = 0.7) +\n  facet_wrap(~ Feature, scales = \"free\") +\n  labs(\n    title = \"Distribution of Selected Features by Tumor Diagnosis\",\n    subtitle = \"Both concave.points_mean and perimeter_mean tend to be higher in malignant (M) tumors\",\n    caption = \"Box plots showing the distribution of concave.points_mean and perimeter_mean by tumor diagnosis.\",\n    x = \"Tumor Diagnosis\",\n    y = \"Feature Value\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nBoth concave.points_mean and perimeter_mean are notably higher in malignant tumors, indicating their potential as diagnostic markers for identifying malignancy."
  },
  {
    "objectID": "example_analysis.html#correlation-analysis",
    "href": "example_analysis.html#correlation-analysis",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "6 Correlation Analysis",
    "text": "6 Correlation Analysis\n#| label: correlation\n#| fig-height: 10\n#| fig-width: 10\n#| warning: false\n# Compute and visualize correlations among numeric features\ncorrelations &lt;- cor(data %&gt;% select(-Diagnosis), method = \"pearson\")\ncorrplot(correlations, number.cex = .6, method = \"number\", type = \"upper\", tl.cex=1, tl.col = \"black\", col = COL2(\"RdYlBu\"))\nSummary: Strong correlations are seen among features like radius, perimeter, and area, indicating redundancy that can be reduced."
  },
  {
    "objectID": "example_analysis.html#references",
    "href": "example_analysis.html#references",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "7 References",
    "text": "7 References\n\n\nShow code\n@misc{wolberg_breast_1993,\n  author = {Wolberg, William H. and Mangasarian, Olvi L.},\n  title = {Breast Cancer Wisconsin (Diagnostic)},\n  year = {1993},\n  publisher = {UCI Machine Learning Repository},\n  note = {\\url{https://doi.org/10.24432/C5DW2B}}\n}\n@misc{ggplot2,\n  author = {Wickham, Hadley},\n  title = {ggplot2: Elegant Graphics for Data Analysis},\n  year = {2016},\n  publisher = {Springer-Verlag New York},\n  note = {\\url{https://ggplot2.tidyverse.org}}\n}\n@misc{corrplot,\n  author = {Wei, Taiyun and Simko, Viliam},\n  title = {corrplot: Visualization of a Correlation Matrix},\n  year = {2017},\n  note = {\\url{https://github.com/taiyun/corrplot}}\n}"
  },
  {
    "objectID": "example_analysis.html#descriptive-statistics",
    "href": "example_analysis.html#descriptive-statistics",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "3 Descriptive Statistics",
    "text": "3 Descriptive Statistics\n\n\nShow code\n# Introduction: Load and inspect the first few rows of the dataset\ndata &lt;- read.csv(\"example_analysis_data.csv\")\ndata &lt;- data %&gt;% select(-id, -X) %&gt;% rename(Diagnosis = diagnosis)\ndata$Diagnosis &lt;- factor(data$Diagnosis, levels = c(\"B\", \"M\"))\nhead(data)\n\n\n\n  \n\n\n\nShow code\n# Summary: The dataset includes various tumor measurements, with a diagnosis column indicating benign or malignant tumors."
  },
  {
    "objectID": "example_analysis.html#target-variable-distribution",
    "href": "example_analysis.html#target-variable-distribution",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "4 Target Variable Distribution",
    "text": "4 Target Variable Distribution\nThis bar plot shows the distribution of benign and malignant diagnoses in the dataset.\n\n\nShow code\n# Plot the distribution of tumor diagnoses\nggplot(data, aes(x = Diagnosis, fill = Diagnosis)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.3) +\n  labs(\n    title = \"Distribution of Tumor Diagnoses in the Dataset\",\n    subtitle = \"Benign tumors are more frequent than malignant tumors\",\n    caption = \"This plot shows the count of benign and malignant tumors in the dataset\",\n    x = \"Tumor Diagnosis\",\n    y = \"Count of Cases\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBenign tumors are more common than malignant tumors, providing a slightly imbalanced but sufficient dataset for training."
  },
  {
    "objectID": "example_analysis.html#data-distribution",
    "href": "example_analysis.html#data-distribution",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "0.5 Data Distribution",
    "text": "0.5 Data Distribution\n\n\nShow code\n# Density and Histogram plots\nplot_density(data, ncol = 5L, ggtheme = theme_minimal())"
  },
  {
    "objectID": "example_analysis.html#box-plot-of-independent-variables-by-diagnosis",
    "href": "example_analysis.html#box-plot-of-independent-variables-by-diagnosis",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "0.6 Box Plot of Independent Variables by Diagnosis",
    "text": "0.6 Box Plot of Independent Variables by Diagnosis\n\n\nShow code\nplot_boxplot(data, by = \"Diagnosis\", ncol = 5L, ggtheme = theme_minimal())"
  },
  {
    "objectID": "example_analysis.html#principal-component-analysis-pca",
    "href": "example_analysis.html#principal-component-analysis-pca",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "6 Principal Component Analysis (PCA)",
    "text": "6 Principal Component Analysis (PCA)\nThe dataset‚Äôs dimensionality was significantly reduced using PCA, which allows the identification and removal of less relevant features without significant loss of information (Hasan and Abdulazeez 2021).\n\n\nShow code\n# Perform PCA for dimensionality reduction\npca &lt;- prcomp(data %&gt;% select(-Diagnosis), scale = TRUE)\n\n# Variance explained by each component\nvar_explained &lt;- data.frame(\n  Component = 1:length(pca$sdev), \n  Variance = (pca$sdev)^2, \n  Proportion = (pca$sdev)^2 / sum((pca$sdev)^2), \n  Cumulative = cumsum((pca$sdev)^2 / sum((pca$sdev)^2))\n)\n\n# Scree Plot\nggplot(var_explained[1:10,], aes(x = Component, y = Proportion)) + \n  geom_bar(stat = \"identity\", fill = \"salmon\") +\n  geom_line(color = \"black\") +\n  geom_point(color = \"black\") +\n  labs(\n    title = \"Explained Variance by Principal Components\",\n    subtitle = \"The first few components capture the majority of variance\",\n    caption = \"Scree plot showing the variance explained by the first 10 components\",\n    x = \"Principal Component\",\n    y = \"Percentage of Explained Variance\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nPCA helped identify six components explaining 88.76% of variance."
  },
  {
    "objectID": "example_analysis.html#data-splitting",
    "href": "example_analysis.html#data-splitting",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "7 Data Splitting",
    "text": "7 Data Splitting\n\n\nShow code\n# Split the data into training and testing sets\nset.seed(101)\nsplit &lt;- sample.split(data$Diagnosis, SplitRatio = 0.8)\ntrain &lt;- subset(data, split == TRUE)\ntest &lt;- subset(data, split == FALSE)\n\n\n\n\nThe training set has 455 samples, and the test set has 114 samples, with consistent diagnosis distribution."
  },
  {
    "objectID": "example_analysis.html#model-training-random-forest",
    "href": "example_analysis.html#model-training-random-forest",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "8 Model Training: Random Forest",
    "text": "8 Model Training: Random Forest\nThe Random Forest method works by constructing a large number of decision trees during training and outputting the most common class (Romano, Barbul, and Korenstein 2023). A Random Forest model with 10-fold cross-validation is trained to predict tumor diagnoses based on the dataset features, using 1000 trees.\n\n\nShow code\n# Set up cross-validation\ncontrol &lt;- trainControl(method = \"cv\", number = 10)\n\n# Random Forest\nrf_model &lt;- train(Diagnosis ~ ., data = train, method = \"rf\", ntree = 1000, trControl = control, importance = TRUE)"
  },
  {
    "objectID": "example_analysis.html#model-evaluation",
    "href": "example_analysis.html#model-evaluation",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "9 Model Evaluation",
    "text": "9 Model Evaluation\n\n\nShow code\n# Evaluate the Random Forest model on the test set\nevaluate_model &lt;- function(model, test_data) {\n  predictions &lt;- predict(model, newdata = test_data)\n  cm &lt;- confusionMatrix(predictions, test_data$Diagnosis)\n  auc_value &lt;- auc(roc(test_data$Diagnosis, as.numeric(predictions)))\n  data.frame(\n    Accuracy = cm$overall['Accuracy'],\n    Sensitivity = cm$byClass['Sensitivity'],\n    Specificity = cm$byClass['Specificity'],\n    AUC = auc_value\n  )\n}\n\nrf_results &lt;- evaluate_model(rf_model, test)\nrf_results\n\n\n\n  \n\n\n\n\n\nThe model achieves high accuracy and AUC, indicating effective classification of benign and malignant cases."
  },
  {
    "objectID": "example_analysis.html#roc-curve-for-random-forest-model",
    "href": "example_analysis.html#roc-curve-for-random-forest-model",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "11 ROC Curve for Random Forest Model",
    "text": "11 ROC Curve for Random Forest Model\n(Romano, Barbul, and Korenstein 2023)\n\n\nShow code\n# Introduction: Plot the ROC curve to visualize model performance\nrf_roc &lt;- roc(test$Diagnosis, as.numeric(predict(rf_model, newdata = test)))\nrf_df &lt;- data.frame(Specificity = 1 - rf_roc$specificities, Sensitivity = rf_roc$sensitivities)\n\n# Plot ROC curve using ggplot2\nggplot(rf_df, aes(x = Specificity, y = Sensitivity)) +\n  geom_line(size = 1, color = \"green\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  labs(\n    title = \"ROC Curve for Random Forest Classification\",\n    subtitle = \"High AUC reflects strong model discrimination ability\",\n    caption = \"ROC curve shows sensitivity vs. specificity for the Random Forest model\",\n    x = \"1 - Specificity (False Positive Rate)\",\n    y = \"Sensitivity (True Positive Rate)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1))\n\n\n\n\n\n\n\n\n\n\n\nThe ROC curve with AUC ~0.98 indicates the Random Forest model‚Äôs high sensitivity and specificity in distinguishing malignant from benign tumors."
  },
  {
    "objectID": "example_analysis.html#functions-used",
    "href": "example_analysis.html#functions-used",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "5 Functions Used",
    "text": "5 Functions Used\n\ndplyr: select, rename, filter, mutate, count, arrange, summarize, across\ntidyr: pivot_longer, drop_na\nggplot2: geom_bar, geom_boxplot, geom_line, geom_point, facet_wrap"
  },
  {
    "objectID": "example_analysis.html#faceted-plot-feature-distribution-by-diagnosis",
    "href": "example_analysis.html#faceted-plot-feature-distribution-by-diagnosis",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "5 Faceted Plot: Feature Distribution by Diagnosis",
    "text": "5 Faceted Plot: Feature Distribution by Diagnosis\nThis faceted histogram compares the distribution of radius_mean between benign and malignant tumors.\n\n\nShow code\n# Compare the distribution of 'radius_mean' across diagnoses using faceting\nggplot(data, aes(x = radius_mean, fill = Diagnosis)) +\n  geom_histogram(binwidth = 0.5, alpha = 0.7, position = \"identity\") +\n  facet_wrap(~ Diagnosis) +\n  labs(\n  title = \"Distribution of Mean Radius by Tumor Diagnosis\",\n  subtitle = \"Malignant tumors tend to have a larger mean radius\",\n  caption = \"Faceted histograms showing 'radius_mean' distribution for each diagnosis\",\n  x = \"Mean Radius\",\n  y = \"Frequency\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nThe faceted histograms reveal that malignant tumors generally have higher mean radius values compared to benign tumors."
  },
  {
    "objectID": "example_analysis.html#checklist",
    "href": "example_analysis.html#checklist",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "6 Checklist",
    "text": "6 Checklist\n\nState the Question: Describe the main question being addressed in the analysis. (Introduction)\nAudience: Identify the intended audience for this analysis. (Introduction)\nData Source: Link to the source of the data and provide a brief description of its origin. (Introduction)\nData Dictionary: Include a link to or display the data dictionary on the webpage. (Data Dictionary)\nData Wrangling: Use at least five unique functions from the dplyr or tidyr package for data wrangling. (Data Wrangling)\nVisualization: Include at least three plots, each with different geom_*() functions from ggplot2 (or equivalent). (Target Variable Distribution, Feature Distribution by Diagnosis, Explained Variance by Principal Components)\nPlot Titles and Labels: Ensure all plots have titles, subtitles, captions, and axis labels that are clear and understandable. (Target Variable Distribution, Feature Distribution by Diagnosis, Explained Variance by Principal Components)\nFaceting: Use facet_grid() or facet_wrap() in at least one plot for segmented views. (Feature Distribution by Diagnosis)\nExternal Image or Table: Include at least one image or table sourced from the web or locally saved (not self-created). (Introduction)\nCallout Blocks: Include at least two distinct callout blocks to emphasize important points. (Introduction, Data Wrangling, Model Evaluation)\nReferences: Use a .bib file with at least three unique citations (e.g., data sources, methods used). (Background, Principal Component Analysis, Random Forest Model Training)\nMargin Content: Add at least one piece of margin content to enhance the analysis. (Target Variable Distribution, Feature Distribution by Diagnosis, Principal Component Analysis)\nSummary: Conclude with a 4-6 sentence paragraph summarizing the analysis results. (Summary)\nFunction List: At the end, list each function used from dplyr, tidyr, and ggplot2 to help verify that all requirements are met. (Functions Used)"
  },
  {
    "objectID": "example_analysis.html#functions-used-1",
    "href": "example_analysis.html#functions-used-1",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "11 Functions Used",
    "text": "11 Functions Used\n\ndplyr: select, rename, filter, mutate, count, arrange\ntidyr: pivot_longer\nggplot2: geom_bar, geom_boxplot, geom_line, geom_point, facet_wrap"
  },
  {
    "objectID": "example_analysis.html#box-plot-feature-distribution-by-diagnosis-concave-points-and-perimeter",
    "href": "example_analysis.html#box-plot-feature-distribution-by-diagnosis-concave-points-and-perimeter",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "5 Box Plot: Feature Distribution by Diagnosis (Concave Points and Perimeter)",
    "text": "5 Box Plot: Feature Distribution by Diagnosis (Concave Points and Perimeter)\nThis box plot compares the distribution of concave.points_mean and perimeter_mean between benign and malignant tumors, both of which are highly correlated with the target variable.\n\n\nShow code\n# Compare the distribution of 'concave.points_mean' and 'perimeter_mean' across diagnoses using a box plot\nggplot(data %&gt;% select(Diagnosis, concave.points_mean, perimeter_mean) %&gt;%\n         pivot_longer(cols = -Diagnosis, names_to = \"Feature\", values_to = \"Value\"), \n       aes(x = Diagnosis, y = Value, fill = Diagnosis)) +\n  geom_boxplot(alpha = 0.7) +\n  facet_wrap(~ Feature, scales = \"free\") +\n  labs(\n    title = \"Distribution of Selected Features by Tumor Diagnosis\",\n    subtitle = \"Both concave.points_mean and perimeter_mean tend to be higher in malignant (M) tumors\",\n    caption = \"Box plots showing the distribution of concave.points_mean and perimeter_mean by tumor diagnosis.\",\n    x = \"Tumor Diagnosis\",\n    y = \"Feature Value\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBoth concave.points_mean and perimeter_mean are notably higher in malignant tumors, indicating their potential as diagnostic markers for identifying malignancy."
  },
  {
    "objectID": "example_analysis.html#background",
    "href": "example_analysis.html#background",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "",
    "text": "This analysis investigates the Breast Cancer Wisconsin (Diagnostic) dataset to answer the question: How effectively can the Random Forest method predict breast cancer diagnosis, and which variables are most crucial in distinguishing between malignant and benign tumors?\nThe intended audience includes medical researchers and clinicians focused on diagnostic features that support early detection and classification of breast cancer.\nThe dataset, originally collected by Dr.¬†William H. Wolberg, is available through the UCI Machine Learning Repository (Wolberg and Mangasarian 1993). The repository hosts detailed tumor measurements essential for assessing feature importance in classification tasks.\nBelow is an image from the Kaggle Breast Cancer Wisconsin (Diagnostic) Data Set website, representing breast cancer cells (Repository 2023).\n\n\n\nBreast Cancer Cells\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis analysis highlights diagnostic features in breast cancer, offering foundational insights that may support the development of advanced machine learning models.\n\n\n\n\n\nThe data dictionary below, detailing each variable‚Äôs description and relevance to the analysis, particularly in identifying differences between benign and malignant tumor characteristics. The original data dictionary also can be found at UCI Machine Learning Repository.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nUnique identifier for each patient\n\n\nDiagnosis\nDiagnosis of the tumor (M = malignant, B = benign)\n\n\nradius_mean\nMean radius: mean of distances from center to points on the perimeter\n\n\ntexture_mean\nMean texture: standard deviation of gray-scale values\n\n\nperimeter_mean\nMean perimeter: mean size of the core tumor perimeter\n\n\narea_mean\nMean area: mean size of the core tumor area\n\n\nsmoothness_mean\nMean smoothness: local variation in radius lengths\n\n\ncompactness_mean\nMean compactness: calculated as (perimeter^2 / area - 1.0)\n\n\nconcavity_mean\nMean concavity: severity of concave portions of the contour\n\n\nconcave.points_mean\nMean concave points: number of concave portions of the contour\n\n\nsymmetry_mean\nMean symmetry: measure of symmetry of cell nucleus\n\n\nfractal_dimension_mean\nMean fractal dimension: ‚Äúcoastline approximation‚Äù - 1\n\n\nradius_se\nStandard error of radius\n\n\ntexture_se\nStandard error of texture\n\n\nperimeter_se\nStandard error of perimeter\n\n\narea_se\nStandard error of area\n\n\nsmoothness_se\nStandard error of smoothness\n\n\ncompactness_se\nStandard error of compactness\n\n\nconcavity_se\nStandard error of concavity\n\n\nconcave.points_se\nStandard error of concave points\n\n\nsymmetry_se\nStandard error of symmetry\n\n\nfractal_dimension_se\nStandard error of fractal dimension\n\n\nradius_worst\nWorst or largest value of radius (mean of the three largest values)\n\n\ntexture_worst\nWorst or largest value of texture\n\n\nperimeter_worst\nWorst or largest value of perimeter\n\n\narea_worst\nWorst or largest value of area\n\n\nsmoothness_worst\nWorst or largest value of smoothness\n\n\ncompactness_worst\nWorst or largest value of compactness\n\n\nconcavity_worst\nWorst or largest value of concavity\n\n\nconcave.points_worst\nWorst or largest value of concave points\n\n\nsymmetry_worst\nWorst or largest value of symmetry\n\n\nfractal_dimension_worst\nWorst or largest value of fractal dimension"
  },
  {
    "objectID": "example_analysis.html#exploratory-data-analysis-eda",
    "href": "example_analysis.html#exploratory-data-analysis-eda",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "2 Exploratory Data Analysis (EDA)",
    "text": "2 Exploratory Data Analysis (EDA)\n\n2.1 Data Wrangling\nThe data wrangling steps refine the Breast Cancer dataset for analysis.\n\n\n\n\n\n\nTip\n\n\n\nCorrelations help identify features highly associated with tumor diagnosis, aiding in feature selection for the model.\n\n\nSteps:\n\nRemoved unnecessary columns (id and X) to focus on diagnostic features.\nRenamed the diagnosis column to Diagnosis for clarity.\nDropped rows with missing values to ensure complete cases.\nConverted the Diagnosis column into a factor with levels ‚ÄúB‚Äù (benign) and ‚ÄúM‚Äù (malignant).\nCalculated correlations with the diagnosis to identify highly predictive variables, retaining only those with correlation &gt; |0.3|.\n\nFunctions Used:\n\nselect(): Excludes specific columns and selects highly correlated variables.\nrename(): Renames diagnosis to Diagnosis for consistency.\ndrop_na(): Removes rows with missing values.\nmutate(): Creates or transforms columns, such as converting Diagnosis to a factor and creating a numeric version for correlation.\nsummarize() + across()**: Calculates correlation values for each feature with Diagnosis.\npivot_longer(): Reshapes the data to make correlation results easier to filter.\narrange(): Sorts correlations by their absolute values.\nfilter(): Selects only variables with a high correlation to the target variable.\n\n\n\nShow code\n# Load data\ndata &lt;- read.csv(\"example_analysis_data.csv\")\n\n# Data wrangling with dplyr and tidyr\ndata &lt;- data %&gt;%\n  select(-id, -X) %&gt;%              \n  rename(Diagnosis = diagnosis) %&gt;% \n  drop_na() %&gt;%                     \n  mutate(Diagnosis = factor(Diagnosis, levels = c(\"B\", \"M\"))) \n\n# Calculate correlations and arrange by correlation with target variable (Diagnosis)\ndata_numeric &lt;- data %&gt;%\n  mutate(Diagnosis_num = as.numeric(Diagnosis) - 1) # Converts factor levels to 0 (B) and 1 (M)\n\ncorrelations &lt;- data_numeric %&gt;%\n  select(-Diagnosis) %&gt;% \n  summarize(across(-Diagnosis_num, ~ cor(., data_numeric$Diagnosis_num, use = \"complete.obs\"))) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"correlation\") %&gt;%\n  arrange(desc(abs(correlation)))\n\n# Print top 10 correlation variables\ntop_10_correlations &lt;- correlations %&gt;% head(10)\ncat(\"Top 10 Variables Most Correlated with Diagnosis:\\n\")\n\n\nTop 10 Variables Most Correlated with Diagnosis:\n\n\nShow code\nprint(top_10_correlations)\n\n\n# A tibble: 10 √ó 2\n   variable             correlation\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 concave.points_worst       0.794\n 2 perimeter_worst            0.783\n 3 concave.points_mean        0.777\n 4 radius_worst               0.776\n 5 perimeter_mean             0.743\n 6 area_worst                 0.734\n 7 radius_mean                0.730\n 8 area_mean                  0.709\n 9 concavity_mean             0.696\n10 concavity_worst            0.660\n\n\nShow code\ncor_threshold &lt;- 0.3\nhigh_corr_vars &lt;- correlations %&gt;%\n  filter(abs(correlation) &gt;= cor_threshold) %&gt;%\n  pull(variable)\n\n# Keep only highly correlated variables in the original data\ndata &lt;- data %&gt;%\n  select(all_of(high_corr_vars), Diagnosis)\n\ncat(\"Dimensions of the Refined Dataset:\\n\")\n\n\nDimensions of the Refined Dataset:\n\n\nShow code\ndim(data)\n\n\n[1] 569  24\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe refined dataset contains 569 rows and 24 columns, focusing on variables with significant correlations to the diagnosis. The ten variables most strongly correlated with breast cancer diagnosis are led by concave.points_worst (0.79) and perimeter_worst (0.78).\n\n\n\n\n2.2 Target Variable Distribution\nThis bar plot shows the distribution of benign and malignant diagnoses in the dataset.\n\n\nShow code\n# Plot the distribution of tumor diagnoses\nggplot(data, aes(x = Diagnosis, fill = Diagnosis)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.3) +\n  labs(\n    title = \"Distribution of Tumor Diagnoses in the Dataset\",\n    subtitle = \"Benign tumors are more frequent than malignant tumors\",\n    caption = \"This plot shows the count of benign and malignant tumors in the dataset\",\n    x = \"Tumor Diagnosis\",\n    y = \"Count of Cases\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBenign tumors are more common than malignant tumors, providing a slightly imbalanced but sufficient dataset for training.\n\n\n2.3 Box Plot: Feature Distribution by Diagnosis (Concave Points and Perimeter)\nThis box plot compares the distribution of concave.points_mean and perimeter_mean between benign and malignant tumors, both of which are highly correlated with the target variable.\n\n\nShow code\n# Compare the distribution of 'concave.points_mean' and 'perimeter_mean' across diagnoses using a box plot\nggplot(data %&gt;% select(Diagnosis, concave.points_mean, perimeter_mean) %&gt;%\n         pivot_longer(cols = -Diagnosis, names_to = \"Feature\", values_to = \"Value\"), \n       aes(x = Diagnosis, y = Value, fill = Diagnosis)) +\n  geom_boxplot(alpha = 0.7) +\n  facet_wrap(~ Feature, scales = \"free\") +\n  labs(\n    title = \"Distribution of Selected Features by Tumor Diagnosis\",\n    subtitle = \"Both concave.points_mean and perimeter_mean tend to be higher in malignant (M) tumors\",\n    caption = \"Box plots showing the distribution of concave.points_mean and perimeter_mean by tumor diagnosis.\",\n    x = \"Tumor Diagnosis\",\n    y = \"Feature Value\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBoth concave.points_mean and perimeter_mean are notably higher in malignant tumors, indicating their potential as diagnostic markers for identifying malignancy."
  },
  {
    "objectID": "example_analysis.html#modeling",
    "href": "example_analysis.html#modeling",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "3 Modeling",
    "text": "3 Modeling\n\n3.1 Principal Component Analysis\nThe dataset‚Äôs dimensionality was significantly reduced using PCA, which allows the identification and removal of less relevant features without significant loss of information (Hasan and Abdulazeez 2021).\n\n# Perform PCA for dimensionality reduction\npca &lt;- prcomp(data %&gt;% select(-Diagnosis), scale = TRUE)\n\n# Variance explained by each component\nvar_explained &lt;- data.frame(\n  Component = 1:length(pca$sdev), \n  Variance = (pca$sdev)^2, \n  Proportion = (pca$sdev)^2 / sum((pca$sdev)^2), \n  Cumulative = cumsum((pca$sdev)^2 / sum((pca$sdev)^2))\n)\n\n# Scree Plot\nggplot(var_explained[1:10,], aes(x = Component)) + \n  geom_bar(aes(y = Proportion), stat = \"identity\", fill = \"salmon\") +\n  geom_line(aes(y = Cumulative), color = \"black\") +\n  geom_point(aes(y = Cumulative), color = \"black\") +\n  labs(\n    title = \"Explained Variance by Principal Components\",\n    subtitle = \"The first few components capture the majority of variance\",\n    caption = \"Scree plot showing the variance explained by the first 10 components and cumulative sum\",\n    x = \"Principal Component\",\n    y = \"Percentage of Explained Variance\"\n  ) +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\n\nPCA helped identify seven components explaining more than 90% of variance.\n\n\n3.2 Data Splitting\nThe PCA-transformed data was split into an 80:20 ratio for the training and test sets.\n\n# Determine the number of components required to explain 90% of the variance\nvar_explained &lt;- cumsum((pca$sdev)^2) / sum((pca$sdev)^2)\nnum_components &lt;- which(var_explained &gt;= 0.9)[1]\npca_data &lt;- as.data.frame(pca$x[, 1:num_components])\npca_data$Diagnosis &lt;- data$Diagnosis\n\n# Split the data into training and testing sets\nset.seed(101)\nsplit &lt;- sample.split(pca_data$Diagnosis, SplitRatio = 0.8)\ntrain_pca &lt;- subset(pca_data, split == TRUE)\ntest_pca &lt;- subset(pca_data, split == FALSE)\n\n\n\n3.3 Random Forest Model Training\nThe Random Forest method works by constructing a large number of decision trees during training and outputting the most common class (Romano, Barbul, and Korenstein 2023). A Random Forest model with 10-fold cross-validation is trained to predict tumor diagnoses based on the dataset features, using 1000 trees.\n\n# Set up cross-validation\ncontrol &lt;- trainControl(method = \"cv\", number = 10)\n\n# Random Forest\nrf_model &lt;- train(Diagnosis ~ ., data = train_pca, method = \"rf\", ntree = 1000, trControl = control, importance = TRUE)\n\n\n\n3.4 Model Evaluation\n\n# Evaluate the Random Forest model on the test set\nevaluate_model &lt;- function(model, test_data) {\n  predictions &lt;- predict(model, newdata = test_data)\n  cm &lt;- confusionMatrix(predictions, test_data$Diagnosis)\n  auc_value &lt;- auc(roc(test_data$Diagnosis, as.numeric(predictions)))\n  data.frame(\n    Accuracy = cm$overall['Accuracy'],\n    Sensitivity = cm$byClass['Sensitivity'],\n    Specificity = cm$byClass['Specificity'],\n    AUC = auc_value\n  )\n}\n\nrf_results &lt;- evaluate_model(rf_model, test_pca)\nrf_results\n\n\n  \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe model achieves high accuracy and AUC, indicating effective classification of benign and malignant cases."
  }
]