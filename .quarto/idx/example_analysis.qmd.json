{"title":"Example Analysis: Breast Cancer Data","markdown":{"yaml":{"title":"Example Analysis: Breast Cancer Data","format":{"html":{"toc":true,"number-sections":true,"fig-cap":true,"df-print":"paged","highlight-style":"tango"}},"bibliography":"references.bib"},"headingText":"Setup chunk for loading libraries","containsRefs":false,"markdown":"\n\n```{r}\n#| echo: false\n#| include: false\n#| warning: false\nlibrary(renv)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(corrplot)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(DataExplorer)\nlibrary(readr)\nlibrary(caTools) \nlibrary(randomForest)\nlibrary(pROC)\n```\n\n## Background\n\n### Introduction\n\nThis analysis investigates the Breast Cancer Wisconsin (Diagnostic) dataset to answer the question: **How effectively can the Random Forest method predict breast cancer diagnosis, and which variables are most crucial in distinguishing between malignant and benign tumors?**\n\nThe intended audience includes **medical researchers and clinicians** focused on diagnostic features that support early detection and classification of breast cancer. \n\nThe dataset, originally collected by Dr. William H. Wolberg, is available through the [UCI Machine Learning Repository](https://doi.org/10.24432/C5DW2B) [@wolberg_breast_1993]. The repository hosts detailed tumor measurements essential for assessing feature importance in classification tasks.\n\nBelow is an image from the Kaggle Breast Cancer Wisconsin (Diagnostic) Data Set website, representing breast cancer cells [@kaggle_breast_cancer_image].\n\n![Breast Cancer Cells](https://storage.googleapis.com/kaggle-datasets-images/180/384/3da2510581f9d3b902307ff8d06fe327/dataset-cover.jpg)\n\n::: {.callout-note}\nThis analysis highlights diagnostic features in breast cancer, offering foundational insights that may support the development of advanced machine learning models.\n:::\n\n### Data Dictionary\n\nThe **data dictionary** below, detailing each variable's description and relevance to the analysis, particularly in identifying differences between benign and malignant tumor characteristics. The original data dictionary also can be found at [UCI Machine Learning Repository](https://doi.org/10.24432/C5DW2B).\n\n| Variable                 | Description                                                                            |\n|--------------------------|----------------------------------------------------------------------------------------|\n| `id`                     | Unique identifier for each patient                                                     |\n| `Diagnosis`              | Diagnosis of the tumor (M = malignant, B = benign)                                     |\n| `radius_mean`            | Mean radius: mean of distances from center to points on the perimeter                  |\n| `texture_mean`           | Mean texture: standard deviation of gray-scale values                                  |\n| `perimeter_mean`         | Mean perimeter: mean size of the core tumor perimeter                                  |\n| `area_mean`              | Mean area: mean size of the core tumor area                                            |\n| `smoothness_mean`        | Mean smoothness: local variation in radius lengths                                     |\n| `compactness_mean`       | Mean compactness: calculated as (perimeter^2 / area - 1.0)                             |\n| `concavity_mean`         | Mean concavity: severity of concave portions of the contour                            |\n| `concave.points_mean`    | Mean concave points: number of concave portions of the contour                         |\n| `symmetry_mean`          | Mean symmetry: measure of symmetry of cell nucleus                                     |\n| `fractal_dimension_mean` | Mean fractal dimension: \"coastline approximation\" - 1                                  |\n| `radius_se`              | Standard error of radius                                                               |\n| `texture_se`             | Standard error of texture                                                              |\n| `perimeter_se`           | Standard error of perimeter                                                            |\n| `area_se`                | Standard error of area                                                                 |\n| `smoothness_se`          | Standard error of smoothness                                                           |\n| `compactness_se`         | Standard error of compactness                                                          |\n| `concavity_se`           | Standard error of concavity                                                            |\n| `concave.points_se`      | Standard error of concave points                                                       |\n| `symmetry_se`            | Standard error of symmetry                                                             |\n| `fractal_dimension_se`   | Standard error of fractal dimension                                                    |\n| `radius_worst`           | Worst or largest value of radius (mean of the three largest values)                    |\n| `texture_worst`          | Worst or largest value of texture                                                      |\n| `perimeter_worst`        | Worst or largest value of perimeter                                                    |\n| `area_worst`             | Worst or largest value of area                                                         |\n| `smoothness_worst`       | Worst or largest value of smoothness                                                   |\n| `compactness_worst`      | Worst or largest value of compactness                                                  |\n| `concavity_worst`        | Worst or largest value of concavity                                                    |\n| `concave.points_worst`   | Worst or largest value of concave points                                               |\n| `symmetry_worst`         | Worst or largest value of symmetry                                                     |\n| `fractal_dimension_worst`| Worst or largest value of fractal dimension                                            |\n\n## Exploratory Data Analysis\n\n### Data Wrangling\n\nThe data wrangling steps refine the Breast Cancer dataset for analysis.\n\n::: {.callout-tip} \nCorrelations help identify features highly associated with tumor diagnosis, aiding in feature selection for the model. \n:::\n\n**Steps:**\n\n1. Removed unnecessary columns (`id` and `X`) to focus on diagnostic features.\n2. Renamed the `diagnosis` column to `Diagnosis` for clarity.\n3. Dropped rows with missing values to ensure complete cases.\n4. Converted the `Diagnosis` column into a factor with levels \"B\" (benign) and \"M\" (malignant).\n5. Calculated correlations with the diagnosis to identify highly predictive variables, retaining only those with correlation > |0.3|.\n\n\n**Functions Used:**\n\n- `select()`: Excludes specific columns and selects highly correlated variables.\n- `rename()`: Renames `diagnosis` to `Diagnosis` for consistency.\n- `drop_na()`: Removes rows with missing values.\n- `mutate()`: Creates or transforms columns, such as converting `Diagnosis` to a factor and creating a numeric version for correlation.\n- `summarize()` + `across()`**: Calculates correlation values for each feature with `Diagnosis`.\n- `pivot_longer()`: Reshapes the data to make correlation results easier to filter.\n- `arrange()`: Sorts correlations by their absolute values.\n- `filter()`: Selects only variables with a high correlation to the target variable.\n\n```{r}\n#| label: descriptive-statistics\n#| echo: true\n#| warning: false\n# Load data\ndata <- read.csv(\"example_analysis_data.csv\")\n\n# Data wrangling with dplyr and tidyr\ndata <- data %>%\n  select(-id, -X) %>%              \n  rename(Diagnosis = diagnosis) %>% \n  drop_na() %>%                     \n  mutate(Diagnosis = factor(Diagnosis, levels = c(\"B\", \"M\"))) \n\n# Calculate correlations and arrange by correlation with target variable (Diagnosis)\ndata_numeric <- data %>%\n  mutate(Diagnosis_num = as.numeric(Diagnosis) - 1) # Converts factor levels to 0 (B) and 1 (M)\n\ncorrelations <- data_numeric %>%\n  select(-Diagnosis) %>% \n  summarize(across(-Diagnosis_num, ~ cor(., data_numeric$Diagnosis_num, use = \"complete.obs\"))) %>%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"correlation\") %>%\n  arrange(desc(abs(correlation)))\n\n# Print top 10 correlation variables\ntop_10_correlations <- correlations %>% head(10)\ncat(\"Top 10 Variables Most Correlated with Diagnosis:\\n\")\nprint(top_10_correlations)\n\ncor_threshold <- 0.3\nhigh_corr_vars <- correlations %>%\n  filter(abs(correlation) >= cor_threshold) %>%\n  pull(variable)\n\n# Keep only highly correlated variables in the original data\ndata <- data %>%\n  select(all_of(high_corr_vars), Diagnosis)\n\ncat(\"Dimensions of the Refined Dataset:\\n\")\ndim(data)\n```\n\n::: {.callout-note}\nThe refined dataset contains 569 rows and 24 columns, focusing on variables with significant correlations to the diagnosis. The ten variables most strongly correlated with breast cancer diagnosis are led by `concave.points_worst` (0.79) and `perimeter_worst` (0.78).\n:::\n\n### Target Variable Distribution\nThis bar plot shows the distribution of benign and malignant diagnoses in the dataset.\n\n```{r}\n#| label: target-variable\n#| fig-height: 6\n#| fig-width: 10\n#| warning: false\n# Plot the distribution of tumor diagnoses\nggplot(data, aes(x = Diagnosis, fill = Diagnosis)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.3) +\n  labs(\n    title = \"Distribution of Tumor Diagnoses in the Dataset\",\n    subtitle = \"Benign tumors are more frequent than malignant tumors\",\n    caption = \"This plot shows the count of benign and malignant tumors in the dataset\",\n    x = \"Tumor Diagnosis\",\n    y = \"Count of Cases\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n```\n\n::: {.column-margin}\nBenign tumors are more common than malignant tumors, providing a slightly imbalanced but sufficient dataset for training.\n::: \n\n### Feature Distribution by Diagnosis\n\nThis box plot compares the distribution of concave.points_mean and perimeter_mean between benign and malignant tumors, both of which are highly correlated with the target variable. \n\n```{r}\n#| label: faceted-plot\n#| fig-height: 6\n#| fig-width: 10\n#| warning: false\n# Compare the distribution of 'concave.points_mean' and 'perimeter_mean' across diagnoses using a box plot\nggplot(data %>% select(Diagnosis, concave.points_mean, perimeter_mean) %>%\n         pivot_longer(cols = -Diagnosis, names_to = \"Feature\", values_to = \"Value\"), \n       aes(x = Diagnosis, y = Value, fill = Diagnosis)) +\n  geom_boxplot(alpha = 0.7) +\n  facet_wrap(~ Feature, scales = \"free\") +\n  labs(\n    title = \"Distribution of Selected Features by Tumor Diagnosis\",\n    subtitle = \"Both concave.points_mean and perimeter_mean tend to be higher in malignant (M) tumors\",\n    caption = \"Box plots showing the distribution of concave.points_mean and perimeter_mean by tumor diagnosis.\",\n    x = \"Tumor Diagnosis\",\n    y = \"Feature Value\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n```\n\n::: {.column-margin}\nBoth concave.points_mean and perimeter_mean are notably higher in malignant tumors, indicating their potential as diagnostic markers for identifying malignancy.\n::: \n\n## Modeling\n\n### Principal Component Analysis\n\nThe dataset's dimensionality was significantly reduced using PCA, which allows the identification and removal of less relevant features without significant loss of information [@hasan2021review].\n\n```{r}\n#| label: pca-analysis\n#| fig-height: 6\n#| fig-width: 10\n#| warning: false\n# Perform PCA for dimensionality reduction\npca <- prcomp(data %>% select(-Diagnosis), scale = TRUE)\n\n# Variance explained by each component\nvar_explained <- data.frame(\n  Component = 1:length(pca$sdev), \n  Variance = (pca$sdev)^2, \n  Proportion = (pca$sdev)^2 / sum((pca$sdev)^2), \n  Cumulative = cumsum((pca$sdev)^2 / sum((pca$sdev)^2))\n)\n\n# Scree Plot\nggplot(var_explained[1:10,], aes(x = Component)) + \n  geom_bar(aes(y = Proportion), stat = \"identity\", fill = \"salmon\") +\n  geom_line(aes(y = Cumulative), color = \"black\") +\n  geom_point(aes(y = Cumulative), color = \"black\") +\n  labs(\n    title = \"Explained Variance by Principal Components\",\n    subtitle = \"The first few components capture the majority of variance\",\n    caption = \"Scree plot showing the variance explained by the first 10 components and cumulative sum\",\n    x = \"Principal Component\",\n    y = \"Percentage of Explained Variance\"\n  ) +\n  scale_y_continuous(labels = scales::percent)\n```\n\n::: {.column-margin}\nPCA helped identify seven components explaining more than 90% of variance.\n:::\n\n### Data Splitting\n\nThe PCA-transformed data was split into an 80:20 ratio for the training and test sets.\n\n```{r}\n#| label: data-split\n#| echo: true\n#| warning: false\n# Determine the number of components required to explain 90% of the variance\nvar_explained <- cumsum((pca$sdev)^2) / sum((pca$sdev)^2)\nnum_components <- which(var_explained >= 0.9)[1]\npca_data <- as.data.frame(pca$x[, 1:num_components])\npca_data$Diagnosis <- data$Diagnosis\n\n# Split the data into training and testing sets\nset.seed(101)\nsplit <- sample.split(pca_data$Diagnosis, SplitRatio = 0.8)\ntrain_pca <- subset(pca_data, split == TRUE)\ntest_pca <- subset(pca_data, split == FALSE)\n```\n\n### Random Forest Model Training\n\nThe Random Forest method works by constructing a large number of decision trees during training and outputting the most common class [@romano2023modeling]. A Random Forest model with 10-fold cross-validation is trained to predict tumor diagnoses based on the dataset features, using 1000 trees.\n\n```{r}\n#| label: model-training\n#| echo: true\n# Set up cross-validation\ncontrol <- trainControl(method = \"cv\", number = 10)\n\n# Random Forest\nrf_model <- train(Diagnosis ~ ., data = train_pca, method = \"rf\", ntree = 1000, trControl = control, importance = TRUE)\n```\n\n### Model Evaluation\n\n```{r}\n#| label: model-evaluation\n#| echo: true\n#| warning: false\n# Evaluate the Random Forest model on the test set\nevaluate_model <- function(model, test_data) {\n  predictions <- predict(model, newdata = test_data)\n  cm <- confusionMatrix(predictions, test_data$Diagnosis)\n  auc_value <- auc(roc(test_data$Diagnosis, as.numeric(predictions)))\n  data.frame(\n    Accuracy = cm$overall['Accuracy'],\n    Sensitivity = cm$byClass['Sensitivity'],\n    Specificity = cm$byClass['Specificity'],\n    AUC = auc_value\n  )\n}\n\nrf_results <- evaluate_model(rf_model, test_pca)\nrf_results\n```\n\n::: {.callout-important}\nThe model achieves high accuracy and AUC, indicating effective classification of benign and malignant cases.\n:::\n\n## Summary\nThe analysis of the Breast Cancer Wisconsin (Diagnostic) dataset revealed that features such as `concave.points_worst`, `perimeter_worst`, and `concave.points_mean` exhibit strong correlations with tumor diagnosis. Dimensionality reduction through PCA indicated that a few principal components account for the majority of variance, efficiently reducing feature redundancy. A Random Forest model, trained and evaluated on the dataset, achieved high accuracy and AUC, demonstrating its effectiveness in tumor classification. These findings provide essential insights into diagnostic markers, supporting potential advancements in early cancer detection and classification models.\n\n## Functions Used\n\n- **dplyr**: `select`, `rename`, `filter`, `mutate`, `count`, `arrange`, `summarize`, `across`\n- **tidyr**: `pivot_longer`, `drop_na`\n- **ggplot2**: `geom_bar`, `geom_boxplot`, `geom_line`, `geom_point`, `facet_wrap`\n\n## Checklist\n\n- [x] **State the Question**: Describe the main question being addressed in the analysis. ([Introduction](#introduction))\n- [x] **Audience**: Identify the intended audience for this analysis. ([Introduction](#introduction))\n- [x] **Data Source**: Link to the source of the data and provide a brief description of its origin. ([Introduction](#introduction))\n- [x] **Data Dictionary**: Include a link to or display the data dictionary on the webpage. ([Data Dictionary](#data-dictionary))\n- [x] **Data Wrangling**: Use at least five unique functions from the `dplyr` or `tidyr` package for data wrangling. ([Data Wrangling](#data-wrangling))\n- [x] **Visualization**: Include at least three plots, each with different `geom_*()` functions from `ggplot2` (or equivalent). ([Target Variable Distribution](#target-variable-distribution), [Feature Distribution by Diagnosis](#feature-distribution-by-diagnosis), [Explained Variance by Principal Components](#explained-variance-by-principal-components))\n- [x] **Plot Titles and Labels**: Ensure all plots have titles, subtitles, captions, and axis labels that are clear and understandable. ([Target Variable Distribution](#target-variable-distribution), [Feature Distribution by Diagnosis](#feature-distribution-by-diagnosis), [Explained Variance by Principal Components](#explained-variance-by-principal-components))\n- [x] **Faceting**: Use `facet_grid()` or `facet_wrap()` in at least one plot for segmented views. ([Feature Distribution by Diagnosis](#feature-distribution-by-diagnosis))\n- [x] **External Image or Table**: Include at least one image or table sourced from the web or locally saved (not self-created). ([Introduction](#introduction))\n- [x] **Callout Blocks**: Include at least two distinct callout blocks to emphasize important points. ([Introduction](#introduction), [Data Wrangling](#data-wrangling), [Model Evaluation](#model-evaluation))\n- [x] **References**: Use a `.bib` file with at least three unique citations (e.g., data sources, methods used). ([Background](#background), [Principal Component Analysis](#principal-component-analysis), [Random Forest Model Training](#random-forest-model-training))\n- [x] **Margin Content**: Add at least one piece of margin content to enhance the analysis. ([Target Variable Distribution](#target-variable-distribution), [Feature Distribution by Diagnosis](#feature-distribution-by-diagnosis), [Principal Component Analysis](#principal-component-analysis))\n- [x] **Summary**: Conclude with a 4-6 sentence paragraph summarizing the analysis results. ([Summary](#summary))\n- [x] **Function List**: At the end, list each function used from `dplyr`, `tidyr`, and `ggplot2` to help verify that all requirements are met. ([Functions Used](#functions-used))","srcMarkdownNoYaml":"\n\n```{r}\n#| echo: false\n#| include: false\n#| warning: false\n# Setup chunk for loading libraries\nlibrary(renv)\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(corrplot)\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(DataExplorer)\nlibrary(readr)\nlibrary(caTools) \nlibrary(randomForest)\nlibrary(pROC)\n```\n\n## Background\n\n### Introduction\n\nThis analysis investigates the Breast Cancer Wisconsin (Diagnostic) dataset to answer the question: **How effectively can the Random Forest method predict breast cancer diagnosis, and which variables are most crucial in distinguishing between malignant and benign tumors?**\n\nThe intended audience includes **medical researchers and clinicians** focused on diagnostic features that support early detection and classification of breast cancer. \n\nThe dataset, originally collected by Dr. William H. Wolberg, is available through the [UCI Machine Learning Repository](https://doi.org/10.24432/C5DW2B) [@wolberg_breast_1993]. The repository hosts detailed tumor measurements essential for assessing feature importance in classification tasks.\n\nBelow is an image from the Kaggle Breast Cancer Wisconsin (Diagnostic) Data Set website, representing breast cancer cells [@kaggle_breast_cancer_image].\n\n![Breast Cancer Cells](https://storage.googleapis.com/kaggle-datasets-images/180/384/3da2510581f9d3b902307ff8d06fe327/dataset-cover.jpg)\n\n::: {.callout-note}\nThis analysis highlights diagnostic features in breast cancer, offering foundational insights that may support the development of advanced machine learning models.\n:::\n\n### Data Dictionary\n\nThe **data dictionary** below, detailing each variable's description and relevance to the analysis, particularly in identifying differences between benign and malignant tumor characteristics. The original data dictionary also can be found at [UCI Machine Learning Repository](https://doi.org/10.24432/C5DW2B).\n\n| Variable                 | Description                                                                            |\n|--------------------------|----------------------------------------------------------------------------------------|\n| `id`                     | Unique identifier for each patient                                                     |\n| `Diagnosis`              | Diagnosis of the tumor (M = malignant, B = benign)                                     |\n| `radius_mean`            | Mean radius: mean of distances from center to points on the perimeter                  |\n| `texture_mean`           | Mean texture: standard deviation of gray-scale values                                  |\n| `perimeter_mean`         | Mean perimeter: mean size of the core tumor perimeter                                  |\n| `area_mean`              | Mean area: mean size of the core tumor area                                            |\n| `smoothness_mean`        | Mean smoothness: local variation in radius lengths                                     |\n| `compactness_mean`       | Mean compactness: calculated as (perimeter^2 / area - 1.0)                             |\n| `concavity_mean`         | Mean concavity: severity of concave portions of the contour                            |\n| `concave.points_mean`    | Mean concave points: number of concave portions of the contour                         |\n| `symmetry_mean`          | Mean symmetry: measure of symmetry of cell nucleus                                     |\n| `fractal_dimension_mean` | Mean fractal dimension: \"coastline approximation\" - 1                                  |\n| `radius_se`              | Standard error of radius                                                               |\n| `texture_se`             | Standard error of texture                                                              |\n| `perimeter_se`           | Standard error of perimeter                                                            |\n| `area_se`                | Standard error of area                                                                 |\n| `smoothness_se`          | Standard error of smoothness                                                           |\n| `compactness_se`         | Standard error of compactness                                                          |\n| `concavity_se`           | Standard error of concavity                                                            |\n| `concave.points_se`      | Standard error of concave points                                                       |\n| `symmetry_se`            | Standard error of symmetry                                                             |\n| `fractal_dimension_se`   | Standard error of fractal dimension                                                    |\n| `radius_worst`           | Worst or largest value of radius (mean of the three largest values)                    |\n| `texture_worst`          | Worst or largest value of texture                                                      |\n| `perimeter_worst`        | Worst or largest value of perimeter                                                    |\n| `area_worst`             | Worst or largest value of area                                                         |\n| `smoothness_worst`       | Worst or largest value of smoothness                                                   |\n| `compactness_worst`      | Worst or largest value of compactness                                                  |\n| `concavity_worst`        | Worst or largest value of concavity                                                    |\n| `concave.points_worst`   | Worst or largest value of concave points                                               |\n| `symmetry_worst`         | Worst or largest value of symmetry                                                     |\n| `fractal_dimension_worst`| Worst or largest value of fractal dimension                                            |\n\n## Exploratory Data Analysis\n\n### Data Wrangling\n\nThe data wrangling steps refine the Breast Cancer dataset for analysis.\n\n::: {.callout-tip} \nCorrelations help identify features highly associated with tumor diagnosis, aiding in feature selection for the model. \n:::\n\n**Steps:**\n\n1. Removed unnecessary columns (`id` and `X`) to focus on diagnostic features.\n2. Renamed the `diagnosis` column to `Diagnosis` for clarity.\n3. Dropped rows with missing values to ensure complete cases.\n4. Converted the `Diagnosis` column into a factor with levels \"B\" (benign) and \"M\" (malignant).\n5. Calculated correlations with the diagnosis to identify highly predictive variables, retaining only those with correlation > |0.3|.\n\n\n**Functions Used:**\n\n- `select()`: Excludes specific columns and selects highly correlated variables.\n- `rename()`: Renames `diagnosis` to `Diagnosis` for consistency.\n- `drop_na()`: Removes rows with missing values.\n- `mutate()`: Creates or transforms columns, such as converting `Diagnosis` to a factor and creating a numeric version for correlation.\n- `summarize()` + `across()`**: Calculates correlation values for each feature with `Diagnosis`.\n- `pivot_longer()`: Reshapes the data to make correlation results easier to filter.\n- `arrange()`: Sorts correlations by their absolute values.\n- `filter()`: Selects only variables with a high correlation to the target variable.\n\n```{r}\n#| label: descriptive-statistics\n#| echo: true\n#| warning: false\n# Load data\ndata <- read.csv(\"example_analysis_data.csv\")\n\n# Data wrangling with dplyr and tidyr\ndata <- data %>%\n  select(-id, -X) %>%              \n  rename(Diagnosis = diagnosis) %>% \n  drop_na() %>%                     \n  mutate(Diagnosis = factor(Diagnosis, levels = c(\"B\", \"M\"))) \n\n# Calculate correlations and arrange by correlation with target variable (Diagnosis)\ndata_numeric <- data %>%\n  mutate(Diagnosis_num = as.numeric(Diagnosis) - 1) # Converts factor levels to 0 (B) and 1 (M)\n\ncorrelations <- data_numeric %>%\n  select(-Diagnosis) %>% \n  summarize(across(-Diagnosis_num, ~ cor(., data_numeric$Diagnosis_num, use = \"complete.obs\"))) %>%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"correlation\") %>%\n  arrange(desc(abs(correlation)))\n\n# Print top 10 correlation variables\ntop_10_correlations <- correlations %>% head(10)\ncat(\"Top 10 Variables Most Correlated with Diagnosis:\\n\")\nprint(top_10_correlations)\n\ncor_threshold <- 0.3\nhigh_corr_vars <- correlations %>%\n  filter(abs(correlation) >= cor_threshold) %>%\n  pull(variable)\n\n# Keep only highly correlated variables in the original data\ndata <- data %>%\n  select(all_of(high_corr_vars), Diagnosis)\n\ncat(\"Dimensions of the Refined Dataset:\\n\")\ndim(data)\n```\n\n::: {.callout-note}\nThe refined dataset contains 569 rows and 24 columns, focusing on variables with significant correlations to the diagnosis. The ten variables most strongly correlated with breast cancer diagnosis are led by `concave.points_worst` (0.79) and `perimeter_worst` (0.78).\n:::\n\n### Target Variable Distribution\nThis bar plot shows the distribution of benign and malignant diagnoses in the dataset.\n\n```{r}\n#| label: target-variable\n#| fig-height: 6\n#| fig-width: 10\n#| warning: false\n# Plot the distribution of tumor diagnoses\nggplot(data, aes(x = Diagnosis, fill = Diagnosis)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.3) +\n  labs(\n    title = \"Distribution of Tumor Diagnoses in the Dataset\",\n    subtitle = \"Benign tumors are more frequent than malignant tumors\",\n    caption = \"This plot shows the count of benign and malignant tumors in the dataset\",\n    x = \"Tumor Diagnosis\",\n    y = \"Count of Cases\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n```\n\n::: {.column-margin}\nBenign tumors are more common than malignant tumors, providing a slightly imbalanced but sufficient dataset for training.\n::: \n\n### Feature Distribution by Diagnosis\n\nThis box plot compares the distribution of concave.points_mean and perimeter_mean between benign and malignant tumors, both of which are highly correlated with the target variable. \n\n```{r}\n#| label: faceted-plot\n#| fig-height: 6\n#| fig-width: 10\n#| warning: false\n# Compare the distribution of 'concave.points_mean' and 'perimeter_mean' across diagnoses using a box plot\nggplot(data %>% select(Diagnosis, concave.points_mean, perimeter_mean) %>%\n         pivot_longer(cols = -Diagnosis, names_to = \"Feature\", values_to = \"Value\"), \n       aes(x = Diagnosis, y = Value, fill = Diagnosis)) +\n  geom_boxplot(alpha = 0.7) +\n  facet_wrap(~ Feature, scales = \"free\") +\n  labs(\n    title = \"Distribution of Selected Features by Tumor Diagnosis\",\n    subtitle = \"Both concave.points_mean and perimeter_mean tend to be higher in malignant (M) tumors\",\n    caption = \"Box plots showing the distribution of concave.points_mean and perimeter_mean by tumor diagnosis.\",\n    x = \"Tumor Diagnosis\",\n    y = \"Feature Value\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n```\n\n::: {.column-margin}\nBoth concave.points_mean and perimeter_mean are notably higher in malignant tumors, indicating their potential as diagnostic markers for identifying malignancy.\n::: \n\n## Modeling\n\n### Principal Component Analysis\n\nThe dataset's dimensionality was significantly reduced using PCA, which allows the identification and removal of less relevant features without significant loss of information [@hasan2021review].\n\n```{r}\n#| label: pca-analysis\n#| fig-height: 6\n#| fig-width: 10\n#| warning: false\n# Perform PCA for dimensionality reduction\npca <- prcomp(data %>% select(-Diagnosis), scale = TRUE)\n\n# Variance explained by each component\nvar_explained <- data.frame(\n  Component = 1:length(pca$sdev), \n  Variance = (pca$sdev)^2, \n  Proportion = (pca$sdev)^2 / sum((pca$sdev)^2), \n  Cumulative = cumsum((pca$sdev)^2 / sum((pca$sdev)^2))\n)\n\n# Scree Plot\nggplot(var_explained[1:10,], aes(x = Component)) + \n  geom_bar(aes(y = Proportion), stat = \"identity\", fill = \"salmon\") +\n  geom_line(aes(y = Cumulative), color = \"black\") +\n  geom_point(aes(y = Cumulative), color = \"black\") +\n  labs(\n    title = \"Explained Variance by Principal Components\",\n    subtitle = \"The first few components capture the majority of variance\",\n    caption = \"Scree plot showing the variance explained by the first 10 components and cumulative sum\",\n    x = \"Principal Component\",\n    y = \"Percentage of Explained Variance\"\n  ) +\n  scale_y_continuous(labels = scales::percent)\n```\n\n::: {.column-margin}\nPCA helped identify seven components explaining more than 90% of variance.\n:::\n\n### Data Splitting\n\nThe PCA-transformed data was split into an 80:20 ratio for the training and test sets.\n\n```{r}\n#| label: data-split\n#| echo: true\n#| warning: false\n# Determine the number of components required to explain 90% of the variance\nvar_explained <- cumsum((pca$sdev)^2) / sum((pca$sdev)^2)\nnum_components <- which(var_explained >= 0.9)[1]\npca_data <- as.data.frame(pca$x[, 1:num_components])\npca_data$Diagnosis <- data$Diagnosis\n\n# Split the data into training and testing sets\nset.seed(101)\nsplit <- sample.split(pca_data$Diagnosis, SplitRatio = 0.8)\ntrain_pca <- subset(pca_data, split == TRUE)\ntest_pca <- subset(pca_data, split == FALSE)\n```\n\n### Random Forest Model Training\n\nThe Random Forest method works by constructing a large number of decision trees during training and outputting the most common class [@romano2023modeling]. A Random Forest model with 10-fold cross-validation is trained to predict tumor diagnoses based on the dataset features, using 1000 trees.\n\n```{r}\n#| label: model-training\n#| echo: true\n# Set up cross-validation\ncontrol <- trainControl(method = \"cv\", number = 10)\n\n# Random Forest\nrf_model <- train(Diagnosis ~ ., data = train_pca, method = \"rf\", ntree = 1000, trControl = control, importance = TRUE)\n```\n\n### Model Evaluation\n\n```{r}\n#| label: model-evaluation\n#| echo: true\n#| warning: false\n# Evaluate the Random Forest model on the test set\nevaluate_model <- function(model, test_data) {\n  predictions <- predict(model, newdata = test_data)\n  cm <- confusionMatrix(predictions, test_data$Diagnosis)\n  auc_value <- auc(roc(test_data$Diagnosis, as.numeric(predictions)))\n  data.frame(\n    Accuracy = cm$overall['Accuracy'],\n    Sensitivity = cm$byClass['Sensitivity'],\n    Specificity = cm$byClass['Specificity'],\n    AUC = auc_value\n  )\n}\n\nrf_results <- evaluate_model(rf_model, test_pca)\nrf_results\n```\n\n::: {.callout-important}\nThe model achieves high accuracy and AUC, indicating effective classification of benign and malignant cases.\n:::\n\n## Summary\nThe analysis of the Breast Cancer Wisconsin (Diagnostic) dataset revealed that features such as `concave.points_worst`, `perimeter_worst`, and `concave.points_mean` exhibit strong correlations with tumor diagnosis. Dimensionality reduction through PCA indicated that a few principal components account for the majority of variance, efficiently reducing feature redundancy. A Random Forest model, trained and evaluated on the dataset, achieved high accuracy and AUC, demonstrating its effectiveness in tumor classification. These findings provide essential insights into diagnostic markers, supporting potential advancements in early cancer detection and classification models.\n\n## Functions Used\n\n- **dplyr**: `select`, `rename`, `filter`, `mutate`, `count`, `arrange`, `summarize`, `across`\n- **tidyr**: `pivot_longer`, `drop_na`\n- **ggplot2**: `geom_bar`, `geom_boxplot`, `geom_line`, `geom_point`, `facet_wrap`\n\n## Checklist\n\n- [x] **State the Question**: Describe the main question being addressed in the analysis. ([Introduction](#introduction))\n- [x] **Audience**: Identify the intended audience for this analysis. ([Introduction](#introduction))\n- [x] **Data Source**: Link to the source of the data and provide a brief description of its origin. ([Introduction](#introduction))\n- [x] **Data Dictionary**: Include a link to or display the data dictionary on the webpage. ([Data Dictionary](#data-dictionary))\n- [x] **Data Wrangling**: Use at least five unique functions from the `dplyr` or `tidyr` package for data wrangling. ([Data Wrangling](#data-wrangling))\n- [x] **Visualization**: Include at least three plots, each with different `geom_*()` functions from `ggplot2` (or equivalent). ([Target Variable Distribution](#target-variable-distribution), [Feature Distribution by Diagnosis](#feature-distribution-by-diagnosis), [Explained Variance by Principal Components](#explained-variance-by-principal-components))\n- [x] **Plot Titles and Labels**: Ensure all plots have titles, subtitles, captions, and axis labels that are clear and understandable. ([Target Variable Distribution](#target-variable-distribution), [Feature Distribution by Diagnosis](#feature-distribution-by-diagnosis), [Explained Variance by Principal Components](#explained-variance-by-principal-components))\n- [x] **Faceting**: Use `facet_grid()` or `facet_wrap()` in at least one plot for segmented views. ([Feature Distribution by Diagnosis](#feature-distribution-by-diagnosis))\n- [x] **External Image or Table**: Include at least one image or table sourced from the web or locally saved (not self-created). ([Introduction](#introduction))\n- [x] **Callout Blocks**: Include at least two distinct callout blocks to emphasize important points. ([Introduction](#introduction), [Data Wrangling](#data-wrangling), [Model Evaluation](#model-evaluation))\n- [x] **References**: Use a `.bib` file with at least three unique citations (e.g., data sources, methods used). ([Background](#background), [Principal Component Analysis](#principal-component-analysis), [Random Forest Model Training](#random-forest-model-training))\n- [x] **Margin Content**: Add at least one piece of margin content to enhance the analysis. ([Target Variable Distribution](#target-variable-distribution), [Feature Distribution by Diagnosis](#feature-distribution-by-diagnosis), [Principal Component Analysis](#principal-component-analysis))\n- [x] **Summary**: Conclude with a 4-6 sentence paragraph summarizing the analysis results. ([Summary](#summary))\n- [x] **Function List**: At the end, list each function used from `dplyr`, `tidyr`, and `ggplot2` to help verify that all requirements are met. ([Functions Used](#functions-used))"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"number-sections":true,"highlight-style":"tango","output-file":"example_analysis.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.32","editor":"visual","theme":{"light":"minty","dark":"solar"},"page-layout":"full","title":"Example Analysis: Breast Cancer Data","bibliography":["references.bib"],"fig-cap":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}