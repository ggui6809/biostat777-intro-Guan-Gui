[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guan Gui",
    "section": "",
    "text": "Hello! I am Guan Gui, a data science and biostatistics student with a passion for uncovering insights from complex biomedical data. I am currently pursuing my ScM in Biostatistics at Johns Hopkins Bloomberg School of Public Health. I enjoy exploring new data science methods and their applications in public health."
  },
  {
    "objectID": "example_analysis.html",
    "href": "example_analysis.html",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "",
    "text": "This analysis investigates the Breast Cancer Wisconsin (Diagnostic) dataset to answer the question: How effectively can the Random Forest method predict breast cancer diagnosis, and which variables are most crucial in distinguishing between malignant and benign tumors?\nThe intended audience includes medical researchers and clinicians focused on diagnostic features that support early detection and classification of breast cancer.\nThe dataset, originally collected by Dr. William H. Wolberg, is available through the UCI Machine Learning Repository (Wolberg and Mangasarian 1993). The repository hosts detailed tumor measurements essential for assessing feature importance in classification tasks.\nBelow is an image from the Kaggle Breast Cancer Wisconsin (Diagnostic) Data Set website, representing breast cancer cells (Repository 2023).\n\n\n\nBreast Cancer Cells\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis analysis highlights diagnostic features in breast cancer, offering foundational insights that may support the development of advanced machine learning models.\n\n\n\n\n\nThe data dictionary below, detailing each variable’s description and relevance to the analysis, particularly in identifying differences between benign and malignant tumor characteristics. The original data dictionary also can be found at UCI Machine Learning Repository.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nUnique identifier for each patient\n\n\nDiagnosis\nDiagnosis of the tumor (M = malignant, B = benign)\n\n\nradius_mean\nMean radius: mean of distances from center to points on the perimeter\n\n\ntexture_mean\nMean texture: standard deviation of gray-scale values\n\n\nperimeter_mean\nMean perimeter: mean size of the core tumor perimeter\n\n\narea_mean\nMean area: mean size of the core tumor area\n\n\nsmoothness_mean\nMean smoothness: local variation in radius lengths\n\n\ncompactness_mean\nMean compactness: calculated as (perimeter^2 / area - 1.0)\n\n\nconcavity_mean\nMean concavity: severity of concave portions of the contour\n\n\nconcave.points_mean\nMean concave points: number of concave portions of the contour\n\n\nsymmetry_mean\nMean symmetry: measure of symmetry of cell nucleus\n\n\nfractal_dimension_mean\nMean fractal dimension: “coastline approximation” - 1\n\n\nradius_se\nStandard error of radius\n\n\ntexture_se\nStandard error of texture\n\n\nperimeter_se\nStandard error of perimeter\n\n\narea_se\nStandard error of area\n\n\nsmoothness_se\nStandard error of smoothness\n\n\ncompactness_se\nStandard error of compactness\n\n\nconcavity_se\nStandard error of concavity\n\n\nconcave.points_se\nStandard error of concave points\n\n\nsymmetry_se\nStandard error of symmetry\n\n\nfractal_dimension_se\nStandard error of fractal dimension\n\n\nradius_worst\nWorst or largest value of radius (mean of the three largest values)\n\n\ntexture_worst\nWorst or largest value of texture\n\n\nperimeter_worst\nWorst or largest value of perimeter\n\n\narea_worst\nWorst or largest value of area\n\n\nsmoothness_worst\nWorst or largest value of smoothness\n\n\ncompactness_worst\nWorst or largest value of compactness\n\n\nconcavity_worst\nWorst or largest value of concavity\n\n\nconcave.points_worst\nWorst or largest value of concave points\n\n\nsymmetry_worst\nWorst or largest value of symmetry\n\n\nfractal_dimension_worst\nWorst or largest value of fractal dimension"
  },
  {
    "objectID": "example_analysis.html#background",
    "href": "example_analysis.html#background",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "",
    "text": "This analysis investigates the Breast Cancer Wisconsin (Diagnostic) dataset to answer the question: How effectively can the Random Forest method predict breast cancer diagnosis, and which variables are most crucial in distinguishing between malignant and benign tumors?\nThe intended audience includes medical researchers and clinicians focused on diagnostic features that support early detection and classification of breast cancer.\nThe dataset, originally collected by Dr. William H. Wolberg, is available through the UCI Machine Learning Repository (Wolberg and Mangasarian 1993). The repository hosts detailed tumor measurements essential for assessing feature importance in classification tasks.\nBelow is an image from the Kaggle Breast Cancer Wisconsin (Diagnostic) Data Set website, representing breast cancer cells (Repository 2023).\n\n\n\nBreast Cancer Cells\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis analysis highlights diagnostic features in breast cancer, offering foundational insights that may support the development of advanced machine learning models.\n\n\n\n\n\nThe data dictionary below, detailing each variable’s description and relevance to the analysis, particularly in identifying differences between benign and malignant tumor characteristics. The original data dictionary also can be found at UCI Machine Learning Repository.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nUnique identifier for each patient\n\n\nDiagnosis\nDiagnosis of the tumor (M = malignant, B = benign)\n\n\nradius_mean\nMean radius: mean of distances from center to points on the perimeter\n\n\ntexture_mean\nMean texture: standard deviation of gray-scale values\n\n\nperimeter_mean\nMean perimeter: mean size of the core tumor perimeter\n\n\narea_mean\nMean area: mean size of the core tumor area\n\n\nsmoothness_mean\nMean smoothness: local variation in radius lengths\n\n\ncompactness_mean\nMean compactness: calculated as (perimeter^2 / area - 1.0)\n\n\nconcavity_mean\nMean concavity: severity of concave portions of the contour\n\n\nconcave.points_mean\nMean concave points: number of concave portions of the contour\n\n\nsymmetry_mean\nMean symmetry: measure of symmetry of cell nucleus\n\n\nfractal_dimension_mean\nMean fractal dimension: “coastline approximation” - 1\n\n\nradius_se\nStandard error of radius\n\n\ntexture_se\nStandard error of texture\n\n\nperimeter_se\nStandard error of perimeter\n\n\narea_se\nStandard error of area\n\n\nsmoothness_se\nStandard error of smoothness\n\n\ncompactness_se\nStandard error of compactness\n\n\nconcavity_se\nStandard error of concavity\n\n\nconcave.points_se\nStandard error of concave points\n\n\nsymmetry_se\nStandard error of symmetry\n\n\nfractal_dimension_se\nStandard error of fractal dimension\n\n\nradius_worst\nWorst or largest value of radius (mean of the three largest values)\n\n\ntexture_worst\nWorst or largest value of texture\n\n\nperimeter_worst\nWorst or largest value of perimeter\n\n\narea_worst\nWorst or largest value of area\n\n\nsmoothness_worst\nWorst or largest value of smoothness\n\n\ncompactness_worst\nWorst or largest value of compactness\n\n\nconcavity_worst\nWorst or largest value of concavity\n\n\nconcave.points_worst\nWorst or largest value of concave points\n\n\nsymmetry_worst\nWorst or largest value of symmetry\n\n\nfractal_dimension_worst\nWorst or largest value of fractal dimension"
  },
  {
    "objectID": "example_analysis.html#exploratory-data-analysis",
    "href": "example_analysis.html#exploratory-data-analysis",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "2 Exploratory Data Analysis",
    "text": "2 Exploratory Data Analysis\n\n2.1 Data Wrangling\nThe data wrangling steps refine the Breast Cancer dataset for analysis.\n\n\n\n\n\n\nTip\n\n\n\nCorrelations help identify features highly associated with tumor diagnosis, aiding in feature selection for the model.\n\n\nSteps:\n\nRemoved unnecessary columns (id and X) to focus on diagnostic features.\nRenamed the diagnosis column to Diagnosis for clarity.\nDropped rows with missing values to ensure complete cases.\nConverted the Diagnosis column into a factor with levels “B” (benign) and “M” (malignant).\nCalculated correlations with the diagnosis to identify highly predictive variables, retaining only those with correlation &gt; |0.3|.\n\nFunctions Used:\n\nselect(): Excludes specific columns and selects highly correlated variables.\nrename(): Renames diagnosis to Diagnosis for consistency.\ndrop_na(): Removes rows with missing values.\nmutate(): Creates or transforms columns, such as converting Diagnosis to a factor and creating a numeric version for correlation.\nsummarize() + across()**: Calculates correlation values for each feature with Diagnosis.\npivot_longer(): Reshapes the data to make correlation results easier to filter.\narrange(): Sorts correlations by their absolute values.\nfilter(): Selects only variables with a high correlation to the target variable.\n\n\n# Load data\ndata &lt;- read.csv(\"example_analysis_data.csv\")\n\n# Data wrangling with dplyr and tidyr\ndata &lt;- data %&gt;%\n  select(-id, -X) %&gt;%              \n  rename(Diagnosis = diagnosis) %&gt;% \n  drop_na() %&gt;%                     \n  mutate(Diagnosis = factor(Diagnosis, levels = c(\"B\", \"M\"))) \n\n# Calculate correlations and arrange by correlation with target variable (Diagnosis)\ndata_numeric &lt;- data %&gt;%\n  mutate(Diagnosis_num = as.numeric(Diagnosis) - 1) # Converts factor levels to 0 (B) and 1 (M)\n\ncorrelations &lt;- data_numeric %&gt;%\n  select(-Diagnosis) %&gt;% \n  summarize(across(-Diagnosis_num, ~ cor(., data_numeric$Diagnosis_num, use = \"complete.obs\"))) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"correlation\") %&gt;%\n  arrange(desc(abs(correlation)))\n\n# Print top 10 correlation variables\ntop_10_correlations &lt;- correlations %&gt;% head(10)\ncat(\"Top 10 Variables Most Correlated with Diagnosis:\\n\")\n\nTop 10 Variables Most Correlated with Diagnosis:\n\nprint(top_10_correlations)\n\n# A tibble: 10 × 2\n   variable             correlation\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 concave.points_worst       0.794\n 2 perimeter_worst            0.783\n 3 concave.points_mean        0.777\n 4 radius_worst               0.776\n 5 perimeter_mean             0.743\n 6 area_worst                 0.734\n 7 radius_mean                0.730\n 8 area_mean                  0.709\n 9 concavity_mean             0.696\n10 concavity_worst            0.660\n\ncor_threshold &lt;- 0.3\nhigh_corr_vars &lt;- correlations %&gt;%\n  filter(abs(correlation) &gt;= cor_threshold) %&gt;%\n  pull(variable)\n\n# Keep only highly correlated variables in the original data\ndata &lt;- data %&gt;%\n  select(all_of(high_corr_vars), Diagnosis)\n\ncat(\"Dimensions of the Refined Dataset:\\n\")\n\nDimensions of the Refined Dataset:\n\ndim(data)\n\n[1] 569  24\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe refined dataset contains 569 rows and 24 columns, focusing on variables with significant correlations to the diagnosis. The ten variables most strongly correlated with breast cancer diagnosis are led by concave.points_worst (0.79) and perimeter_worst (0.78).\n\n\n\n\n2.2 Target Variable Distribution\nThis bar plot shows the distribution of benign and malignant diagnoses in the dataset.\n\n# Plot the distribution of tumor diagnoses\nggplot(data, aes(x = Diagnosis, fill = Diagnosis)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.3) +\n  labs(\n    title = \"Distribution of Tumor Diagnoses in the Dataset\",\n    subtitle = \"Benign tumors are more frequent than malignant tumors\",\n    caption = \"This plot shows the count of benign and malignant tumors in the dataset\",\n    x = \"Tumor Diagnosis\",\n    y = \"Count of Cases\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nBenign tumors are more common than malignant tumors, providing a slightly imbalanced but sufficient dataset for training.\n\n\n2.3 Feature Distribution by Diagnosis\nThis box plot compares the distribution of concave.points_mean and perimeter_mean between benign and malignant tumors, both of which are highly correlated with the target variable.\n\n# Compare the distribution of 'concave.points_mean' and 'perimeter_mean' across diagnoses using a box plot\nggplot(data %&gt;% select(Diagnosis, concave.points_mean, perimeter_mean) %&gt;%\n         pivot_longer(cols = -Diagnosis, names_to = \"Feature\", values_to = \"Value\"), \n       aes(x = Diagnosis, y = Value, fill = Diagnosis)) +\n  geom_boxplot(alpha = 0.7) +\n  facet_wrap(~ Feature, scales = \"free\") +\n  labs(\n    title = \"Distribution of Selected Features by Tumor Diagnosis\",\n    subtitle = \"Both concave.points_mean and perimeter_mean tend to be higher in malignant (M) tumors\",\n    caption = \"Box plots showing the distribution of concave.points_mean and perimeter_mean by tumor diagnosis.\",\n    x = \"Tumor Diagnosis\",\n    y = \"Feature Value\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nBoth concave.points_mean and perimeter_mean are notably higher in malignant tumors, indicating their potential as diagnostic markers for identifying malignancy."
  },
  {
    "objectID": "example_analysis.html#modeling",
    "href": "example_analysis.html#modeling",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "3 Modeling",
    "text": "3 Modeling\n\n3.1 Principal Component Analysis\nThe dataset’s dimensionality was significantly reduced using PCA, which allows the identification and removal of less relevant features without significant loss of information (Hasan and Abdulazeez 2021).\n\n# Perform PCA for dimensionality reduction\npca &lt;- prcomp(data %&gt;% select(-Diagnosis), scale = TRUE)\n\n# Variance explained by each component\nvar_explained &lt;- data.frame(\n  Component = 1:length(pca$sdev), \n  Variance = (pca$sdev)^2, \n  Proportion = (pca$sdev)^2 / sum((pca$sdev)^2), \n  Cumulative = cumsum((pca$sdev)^2 / sum((pca$sdev)^2))\n)\n\n# Scree Plot\nggplot(var_explained[1:10,], aes(x = Component)) + \n  geom_bar(aes(y = Proportion), stat = \"identity\", fill = \"salmon\") +\n  geom_line(aes(y = Cumulative), color = \"black\") +\n  geom_point(aes(y = Cumulative), color = \"black\") +\n  labs(\n    title = \"Explained Variance by Principal Components\",\n    subtitle = \"The first few components capture the majority of variance\",\n    caption = \"Scree plot showing the variance explained by the first 10 components and cumulative sum\",\n    x = \"Principal Component\",\n    y = \"Percentage of Explained Variance\"\n  ) +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\n\nPCA helped identify seven components explaining more than 90% of variance.\n\n\n3.2 Data Splitting\nThe PCA-transformed data was split into an 80:20 ratio for the training and test sets.\n\n# Determine the number of components required to explain 90% of the variance\nvar_explained &lt;- cumsum((pca$sdev)^2) / sum((pca$sdev)^2)\nnum_components &lt;- which(var_explained &gt;= 0.9)[1]\npca_data &lt;- as.data.frame(pca$x[, 1:num_components])\npca_data$Diagnosis &lt;- data$Diagnosis\n\n# Split the data into training and testing sets\nset.seed(101)\nsplit &lt;- sample.split(pca_data$Diagnosis, SplitRatio = 0.8)\ntrain_pca &lt;- subset(pca_data, split == TRUE)\ntest_pca &lt;- subset(pca_data, split == FALSE)\n\n\n\n3.3 Random Forest Model Training\nThe Random Forest method works by constructing a large number of decision trees during training and outputting the most common class (Romano, Barbul, and Korenstein 2023). A Random Forest model with 10-fold cross-validation is trained to predict tumor diagnoses based on the dataset features, using 1000 trees.\n\n# Set up cross-validation\ncontrol &lt;- trainControl(method = \"cv\", number = 10)\n\n# Random Forest\nrf_model &lt;- train(Diagnosis ~ ., data = train_pca, method = \"rf\", ntree = 1000, trControl = control, importance = TRUE)\n\n\n\n3.4 Model Evaluation\n\n# Evaluate the Random Forest model on the test set\nevaluate_model &lt;- function(model, test_data) {\n  predictions &lt;- predict(model, newdata = test_data)\n  cm &lt;- confusionMatrix(predictions, test_data$Diagnosis)\n  auc_value &lt;- auc(roc(test_data$Diagnosis, as.numeric(predictions)))\n  data.frame(\n    Accuracy = cm$overall['Accuracy'],\n    Sensitivity = cm$byClass['Sensitivity'],\n    Specificity = cm$byClass['Specificity'],\n    AUC = auc_value\n  )\n}\n\nrf_results &lt;- evaluate_model(rf_model, test_pca)\nrf_results\n\n\n  \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe model achieves high accuracy and AUC, indicating effective classification of benign and malignant cases."
  },
  {
    "objectID": "example_analysis.html#summary",
    "href": "example_analysis.html#summary",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "4 Summary",
    "text": "4 Summary\nThe analysis of the Breast Cancer Wisconsin (Diagnostic) dataset revealed that features such as concave.points_worst, perimeter_worst, and concave.points_mean exhibit strong correlations with tumor diagnosis. Dimensionality reduction through PCA indicated that a few principal components account for the majority of variance, efficiently reducing feature redundancy. A Random Forest model, trained and evaluated on the dataset, achieved high accuracy and AUC, demonstrating its effectiveness in tumor classification. These findings provide essential insights into diagnostic markers, supporting potential advancements in early cancer detection and classification models."
  },
  {
    "objectID": "example_analysis.html#functions-used",
    "href": "example_analysis.html#functions-used",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "5 Functions Used",
    "text": "5 Functions Used\n\ndplyr: select, rename, filter, mutate, count, arrange, summarize, across\ntidyr: pivot_longer, drop_na\nggplot2: geom_bar, geom_boxplot, geom_line, geom_point, facet_wrap"
  },
  {
    "objectID": "example_analysis.html#checklist",
    "href": "example_analysis.html#checklist",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "6 Checklist",
    "text": "6 Checklist\n\nState the Question: Describe the main question being addressed in the analysis. (Introduction)\nAudience: Identify the intended audience for this analysis. (Introduction)\nData Source: Link to the source of the data and provide a brief description of its origin. (Introduction)\nData Dictionary: Include a link to or display the data dictionary on the webpage. (Data Dictionary)\nData Wrangling: Use at least five unique functions from the dplyr or tidyr package for data wrangling. (Data Wrangling)\nVisualization: Include at least three plots, each with different geom_*() functions from ggplot2 (or equivalent). (Target Variable Distribution, Feature Distribution by Diagnosis, Explained Variance by Principal Components)\nPlot Titles and Labels: Ensure all plots have titles, subtitles, captions, and axis labels that are clear and understandable. (Target Variable Distribution, Feature Distribution by Diagnosis, Explained Variance by Principal Components)\nFaceting: Use facet_grid() or facet_wrap() in at least one plot for segmented views. (Feature Distribution by Diagnosis)\nExternal Image or Table: Include at least one image or table sourced from the web or locally saved (not self-created). (Introduction)\nCallout Blocks: Include at least two distinct callout blocks to emphasize important points. (Introduction, Data Wrangling, Model Evaluation)\nReferences: Use a .bib file with at least three unique citations (e.g., data sources, methods used). (Background, Principal Component Analysis, Random Forest Model Training)\nMargin Content: Add at least one piece of margin content to enhance the analysis. (Target Variable Distribution, Feature Distribution by Diagnosis, Principal Component Analysis)\nSummary: Conclude with a 4-6 sentence paragraph summarizing the analysis results. (Summary)\nFunction List: At the end, list each function used from dplyr, tidyr, and ggplot2 to help verify that all requirements are met. (Functions Used)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently an ScM student in Biostatistics at Johns Hopkins Bloomberg School of Public Health, following my completion of a first-class honours degree in Data Science at the University of Sydney. My research interests span biostatistics, spatial genomics, and machine learning, with a focus on analyzing and deriving insights from complex data.\n\n\n\nMaster of Science in Biostatistics (ScM) Sep. 2024 - Present\n📍 Baltimore, MD\nJohns Hopkins Bloomberg School of Public Health\nBachelor of Science (Honours) in Data Science Mar. 2021 - Jul. 2024\n📍 Sydney, Australia\nUniversity of Sydney\nGraduated with First Class Honours\nAwards: Vice Chancellor’s Global Mobility Scholarship, Dalyell Scholar, Charles Perkins Centre Summer Research Scholarship\nSemester Exchange Aug. 2022 - Dec. 2022\n📍 Singapore\nNational University of Singapore\nSummer & Winter Schools Dec. 2021, Jul. 2022\n📍 Shanghai, China\nShanghai Jiao Tong University\n\n\n\n\n\nSpatialFeatures Project Jul. 2023 - Present\n📍 Sydney, Australia\nUniversity of Sydney\nDeveloped the “SpatialFeatures” algorithm to analyze spatial genomics data at cellular and sub-cellular levels, creating a versatile R package for use across genomics datasets.\nData Analyst Intern Jan. - May 2022\n📍 Shanghai, China\nDiAct Technology Co., Ltd. (Ipsos)\nAnalyzed social media data for automotive trends, improving data processing by 30% through Python optimizations.\nUser Growth Intern Jul. - Oct. 2021\n📍 Beijing, China\nJD.com\nEnhanced SQL processes and developed metrics for improved user engagement recommendations.\n\n\n\n\n\nProgramming: R, Python, SQL, SAS, Java, Tableau\nLanguages: Chinese (Native), English (Fluent)\n\n\n\n\nOutside of academics, I love fitness and playing Apex Legends."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Master of Science in Biostatistics (ScM) Sep. 2024 - Present\n📍 Baltimore, MD\nJohns Hopkins Bloomberg School of Public Health\nBachelor of Science (Honours) in Data Science Mar. 2021 - Jul. 2024\n📍 Sydney, Australia\nUniversity of Sydney\nGraduated with First Class Honours\nAwards: Vice Chancellor’s Global Mobility Scholarship, Dalyell Scholar, Charles Perkins Centre Summer Research Scholarship\nSemester Exchange Aug. 2022 - Dec. 2022\n📍 Singapore\nNational University of Singapore\nSummer & Winter Schools Dec. 2021, Jul. 2022\n📍 Shanghai, China\nShanghai Jiao Tong University"
  },
  {
    "objectID": "about.html#research-and-professional-experience",
    "href": "about.html#research-and-professional-experience",
    "title": "About",
    "section": "",
    "text": "SpatialFeatures Project Jul. 2023 - Present\n📍 Sydney, Australia\nUniversity of Sydney\nDeveloped the “SpatialFeatures” algorithm to analyze spatial genomics data at cellular and sub-cellular levels, creating a versatile R package for use across genomics datasets.\nData Analyst Intern Jan. - May 2022\n📍 Shanghai, China\nDiAct Technology Co., Ltd. (Ipsos)\nAnalyzed social media data for automotive trends, improving data processing by 30% through Python optimizations.\nUser Growth Intern Jul. - Oct. 2021\n📍 Beijing, China\nJD.com\nEnhanced SQL processes and developed metrics for improved user engagement recommendations."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "",
    "text": "Programming: R, Python, SQL, SAS, Java, Tableau\nLanguages: Chinese (Native), English (Fluent)"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About",
    "section": "",
    "text": "Outside of academics, I love fitness and playing Apex Legends."
  }
]