[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guan Gui",
    "section": "",
    "text": "Hello! I am Guan Gui, a data science and biostatistics student with a passion for uncovering insights from complex biomedical data. I am currently pursuing my ScM in Biostatistics at Johns Hopkins Bloomberg School of Public Health. I enjoy exploring new data science methods and their applications in public health."
  },
  {
    "objectID": "example_analysis.html",
    "href": "example_analysis.html",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "",
    "text": "This analysis investigates the Breast Cancer Wisconsin (Diagnostic) dataset to determine which features are most effective in distinguishing malignant from benign tumors. Specifically, we aim to answer the question: What variables are most influential in predicting breast cancer diagnosis?\nThis analysis is intended for medical researchers and clinicians interested in exploring diagnostic features that can assist in early detection and classification of breast cancer tumors. The dataset was originally collected by Dr. William H. Wolberg and is available from the UCI Machine Learning Repository (Wolberg and Mangasarian 1993).\nA complete data dictionary is provided below.\n\n\n\n\n\n\nNote\n\n\n\nThis analysis provides insights into diagnostic features in breast cancer, potentially guiding more advanced machine learning models."
  },
  {
    "objectID": "example_analysis.html#introduction",
    "href": "example_analysis.html#introduction",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "",
    "text": "This analysis investigates the Breast Cancer Wisconsin (Diagnostic) dataset to determine which features are most effective in distinguishing malignant from benign tumors. Specifically, we aim to answer the question: What variables are most influential in predicting breast cancer diagnosis?\nThis analysis is intended for medical researchers and clinicians interested in exploring diagnostic features that can assist in early detection and classification of breast cancer tumors. The dataset was originally collected by Dr. William H. Wolberg and is available from the UCI Machine Learning Repository (Wolberg and Mangasarian 1993).\nA complete data dictionary is provided below.\n\n\n\n\n\n\nNote\n\n\n\nThis analysis provides insights into diagnostic features in breast cancer, potentially guiding more advanced machine learning models."
  },
  {
    "objectID": "example_analysis.html#data-dictionary",
    "href": "example_analysis.html#data-dictionary",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "2 Data Dictionary",
    "text": "2 Data Dictionary\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nUnique identifier for each patient\n\n\nDiagnosis\nDiagnosis of the tumor (M = malignant, B = benign)\n\n\nradius_mean\nMean radius: mean of distances from center to points on the perimeter\n\n\ntexture_mean\nMean texture: standard deviation of gray-scale values\n\n\nperimeter_mean\nMean perimeter: mean size of the core tumor perimeter\n\n\narea_mean\nMean area: mean size of the core tumor area\n\n\nsmoothness_mean\nMean smoothness: local variation in radius lengths\n\n\ncompactness_mean\nMean compactness: calculated as (perimeter^2 / area - 1.0)\n\n\nconcavity_mean\nMean concavity: severity of concave portions of the contour\n\n\nconcave.points_mean\nMean concave points: number of concave portions of the contour\n\n\nsymmetry_mean\nMean symmetry: measure of symmetry of cell nucleus\n\n\nfractal_dimension_mean\nMean fractal dimension: “coastline approximation” - 1\n\n\nradius_se\nStandard error of radius\n\n\ntexture_se\nStandard error of texture\n\n\nperimeter_se\nStandard error of perimeter\n\n\narea_se\nStandard error of area\n\n\nsmoothness_se\nStandard error of smoothness\n\n\ncompactness_se\nStandard error of compactness\n\n\nconcavity_se\nStandard error of concavity\n\n\nconcave.points_se\nStandard error of concave points\n\n\nsymmetry_se\nStandard error of symmetry\n\n\nfractal_dimension_se\nStandard error of fractal dimension\n\n\nradius_worst\nWorst or largest value of radius (mean of the three largest values)\n\n\ntexture_worst\nWorst or largest value of texture\n\n\nperimeter_worst\nWorst or largest value of perimeter\n\n\narea_worst\nWorst or largest value of area\n\n\nsmoothness_worst\nWorst or largest value of smoothness\n\n\ncompactness_worst\nWorst or largest value of compactness\n\n\nconcavity_worst\nWorst or largest value of concavity\n\n\nconcave.points_worst\nWorst or largest value of concave points\n\n\nsymmetry_worst\nWorst or largest value of symmetry\n\n\nfractal_dimension_worst\nWorst or largest value of fractal dimension\n\n\n\nFor a complete data dictionary, refer to the UCI Repository’s dataset description.\nBelow is an image from the Kaggle Breast Cancer Wisconsin (Diagnostic) Data Set, representing breast cancer cells (Repository 2023).\n\n\n\nBreast Cancer Cells"
  },
  {
    "objectID": "example_analysis.html#data-wrangling",
    "href": "example_analysis.html#data-wrangling",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "3 Data Wrangling",
    "text": "3 Data Wrangling\nThe data wrangling steps refine the Breast Cancer dataset for analysis.\n\n\n\n\n\n\nTip\n\n\n\nCorrelations help identify features highly associated with tumor diagnosis, aiding in feature selection for the model.\n\n\nSteps:\n\nRemoved unnecessary columns (id and X) to focus on diagnostic features.\nRenamed the diagnosis column to Diagnosis for clarity.\nDropped rows with missing values to ensure complete cases.\nConverted the Diagnosis column into a factor with levels “B” (benign) and “M” (malignant).\nCalculated correlations with the diagnosis to identify highly predictive variables, retaining only those with correlation &gt; |0.3|.\n\n\n3.1 Functions Used\n\nselect(): Excludes specific columns and selects highly correlated variables.\nrename(): Renames diagnosis to Diagnosis for consistency.\ndrop_na(): Removes rows with missing values.\nmutate(): Creates or transforms columns, such as converting Diagnosis to a factor and creating a numeric version for correlation.\nsummarize() + across(): Calculates correlation values for each feature with Diagnosis.\npivot_longer(): Reshapes the data to make correlation results easier to filter.\narrange(): Sorts correlations by their absolute values.\nfilter(): Selects only variables with a high correlation to the target variable.\n\n\n\nShow code\n# Load data\ndata &lt;- read.csv(\"example_analysis_data.csv\")\n\n# Data wrangling with dplyr and tidyr\ndata &lt;- data %&gt;%\n  select(-id, -X) %&gt;%              \n  rename(Diagnosis = diagnosis) %&gt;% \n  drop_na() %&gt;%                     \n  mutate(Diagnosis = factor(Diagnosis, levels = c(\"B\", \"M\"))) \n\n# Calculate correlations and arrange by correlation with target variable (Diagnosis)\ndata_numeric &lt;- data %&gt;%\n  mutate(Diagnosis_num = as.numeric(Diagnosis) - 1) # Converts factor levels to 0 (B) and 1 (M)\n\ncorrelations &lt;- data_numeric %&gt;%\n  select(-Diagnosis) %&gt;% \n  summarize(across(-Diagnosis_num, ~ cor(., data_numeric$Diagnosis_num, use = \"complete.obs\"))) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"correlation\") %&gt;%\n  arrange(desc(abs(correlation)))\n\n# Print top 10 correlation variables\ntop_10_correlations &lt;- correlations %&gt;% head(10)\nprint(top_10_correlations)\n\n\n# A tibble: 10 × 2\n   variable             correlation\n   &lt;chr&gt;                      &lt;dbl&gt;\n 1 concave.points_worst       0.794\n 2 perimeter_worst            0.783\n 3 concave.points_mean        0.777\n 4 radius_worst               0.776\n 5 perimeter_mean             0.743\n 6 area_worst                 0.734\n 7 radius_mean                0.730\n 8 area_mean                  0.709\n 9 concavity_mean             0.696\n10 concavity_worst            0.660\n\n\nShow code\ncor_threshold &lt;- 0.3\nhigh_corr_vars &lt;- correlations %&gt;%\n  filter(abs(correlation) &gt;= cor_threshold) %&gt;%\n  pull(variable)\n\n# Keep only highly correlated variables in the original data\ndata &lt;- data %&gt;%\n  select(all_of(high_corr_vars), Diagnosis)\n\nhead(data)"
  },
  {
    "objectID": "example_analysis.html#target-variable-distribution",
    "href": "example_analysis.html#target-variable-distribution",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "4 Target Variable Distribution",
    "text": "4 Target Variable Distribution\nThis bar plot shows the distribution of benign and malignant diagnoses in the dataset.\n\n\nShow code\n# Plot the distribution of tumor diagnoses\nggplot(data, aes(x = Diagnosis, fill = Diagnosis)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), vjust=-0.3) +\n  labs(\n    title = \"Distribution of Tumor Diagnoses in the Dataset\",\n    subtitle = \"Benign tumors are more frequent than malignant tumors\",\n    caption = \"This plot shows the count of benign and malignant tumors in the dataset\",\n    x = \"Tumor Diagnosis\",\n    y = \"Count of Cases\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nBenign tumors are more common than malignant tumors, providing a slightly imbalanced but sufficient dataset for training."
  },
  {
    "objectID": "example_analysis.html#faceted-plot-feature-distribution-by-diagnosis",
    "href": "example_analysis.html#faceted-plot-feature-distribution-by-diagnosis",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "5 Faceted Plot: Feature Distribution by Diagnosis",
    "text": "5 Faceted Plot: Feature Distribution by Diagnosis\nThis faceted histogram compares the distribution of radius_mean between benign and malignant tumors.\n\n\nShow code\n# Compare the distribution of 'radius_mean' across diagnoses using faceting\nggplot(data, aes(x = radius_mean, fill = Diagnosis)) +\n  geom_histogram(binwidth = 0.5, alpha = 0.7, position = \"identity\") +\n  facet_wrap(~ Diagnosis) +\n  labs(\n  title = \"Distribution of Mean Radius by Tumor Diagnosis\",\n  subtitle = \"Malignant tumors tend to have a larger mean radius\",\n  caption = \"Faceted histograms showing 'radius_mean' distribution for each diagnosis\",\n  x = \"Mean Radius\",\n  y = \"Frequency\"\n  ) +\n  scale_fill_manual(values = c(\"B\" = \"#00CCCC\", \"M\" = \"salmon\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nThe faceted histograms reveal that malignant tumors generally have higher mean radius values compared to benign tumors."
  },
  {
    "objectID": "example_analysis.html#correlation-analysis",
    "href": "example_analysis.html#correlation-analysis",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "6 Correlation Analysis",
    "text": "6 Correlation Analysis\n#| label: correlation\n#| fig-height: 10\n#| fig-width: 10\n#| warning: false\n# Compute and visualize correlations among numeric features\ncorrelations &lt;- cor(data %&gt;% select(-Diagnosis), method = \"pearson\")\ncorrplot(correlations, number.cex = .6, method = \"number\", type = \"upper\", tl.cex=1, tl.col = \"black\", col = COL2(\"RdYlBu\"))\nSummary: Strong correlations are seen among features like radius, perimeter, and area, indicating redundancy that can be reduced."
  },
  {
    "objectID": "example_analysis.html#principal-component-analysis-pca",
    "href": "example_analysis.html#principal-component-analysis-pca",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "7 Principal Component Analysis (PCA)",
    "text": "7 Principal Component Analysis (PCA)\nThe dataset’s dimensionality was significantly reduced using PCA, which allows the identification and removal of less relevant features without significant loss of information (Hasan and Abdulazeez 2021).\n\n\nShow code\n# Perform PCA for dimensionality reduction\npca &lt;- prcomp(data %&gt;% select(-Diagnosis), scale = TRUE)\n\n# Variance explained by each component\nvar_explained &lt;- data.frame(\n  Component = 1:length(pca$sdev), \n  Variance = (pca$sdev)^2, \n  Proportion = (pca$sdev)^2 / sum((pca$sdev)^2), \n  Cumulative = cumsum((pca$sdev)^2 / sum((pca$sdev)^2))\n)\n\n# Scree Plot\nggplot(var_explained[1:10,], aes(x = Component, y = Proportion)) + \n  geom_bar(stat = \"identity\", fill = \"salmon\") +\n  geom_line(color = \"black\") +\n  geom_point(color = \"black\") +\n  labs(\n    title = \"Explained Variance by Principal Components\",\n    subtitle = \"The first few components capture the majority of variance\",\n    caption = \"Scree plot showing the variance explained by the first 10 components\",\n    x = \"Principal Component\",\n    y = \"Percentage of Explained Variance\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nPCA helped identify six components explaining 88.76% of variance."
  },
  {
    "objectID": "example_analysis.html#data-splitting",
    "href": "example_analysis.html#data-splitting",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "8 Data Splitting",
    "text": "8 Data Splitting\n\n\nShow code\n# Split the data into training and testing sets\nset.seed(101)\nsplit &lt;- sample.split(data$Diagnosis, SplitRatio = 0.8)\ntrain &lt;- subset(data, split == TRUE)\ntest &lt;- subset(data, split == FALSE)\n\n\n\n\nThe training set has 455 samples, and the test set has 114 samples, with consistent diagnosis distribution."
  },
  {
    "objectID": "example_analysis.html#model-training-random-forest",
    "href": "example_analysis.html#model-training-random-forest",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "9 Model Training: Random Forest",
    "text": "9 Model Training: Random Forest\nThe Random Forest method works by constructing a large number of decision trees during training and outputting the most common class (Romano, Barbul, and Korenstein 2023). A Random Forest model with 10-fold cross-validation is trained to predict tumor diagnoses based on the dataset features, using 1000 trees.\n\n\nShow code\n# Set up cross-validation\ncontrol &lt;- trainControl(method = \"cv\", number = 10)\n\n# Random Forest\nrf_model &lt;- train(Diagnosis ~ ., data = train, method = \"rf\", ntree = 1000, trControl = control, importance = TRUE)"
  },
  {
    "objectID": "example_analysis.html#model-evaluation",
    "href": "example_analysis.html#model-evaluation",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "10 Model Evaluation",
    "text": "10 Model Evaluation\n\n\nShow code\n# Evaluate the Random Forest model on the test set\nevaluate_model &lt;- function(model, test_data) {\n  predictions &lt;- predict(model, newdata = test_data)\n  cm &lt;- confusionMatrix(predictions, test_data$Diagnosis)\n  auc_value &lt;- auc(roc(test_data$Diagnosis, as.numeric(predictions)))\n  data.frame(\n    Accuracy = cm$overall['Accuracy'],\n    Sensitivity = cm$byClass['Sensitivity'],\n    Specificity = cm$byClass['Specificity'],\n    AUC = auc_value\n  )\n}\n\nrf_results &lt;- evaluate_model(rf_model, test)\nrf_results\n\n\n\n  \n\n\n\n\n\nThe model achieves high accuracy and AUC, indicating effective classification of benign and malignant cases."
  },
  {
    "objectID": "example_analysis.html#summary",
    "href": "example_analysis.html#summary",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "11 Summary",
    "text": "11 Summary\nThe analysis of the Breast Cancer Wisconsin (Diagnostic) dataset revealed that features such as concave.points_worst, perimeter_worst, and concave.points_mean exhibit strong correlations with tumor diagnosis. Dimensionality reduction through PCA indicated that a few principal components account for the majority of variance, efficiently reducing feature redundancy. A Random Forest model, trained and evaluated on the dataset, achieved high accuracy and AUC, demonstrating its effectiveness in tumor classification. These findings provide essential insights into diagnostic markers, supporting potential advancements in early cancer detection and classification models."
  },
  {
    "objectID": "example_analysis.html#functions-used-1",
    "href": "example_analysis.html#functions-used-1",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "12 Functions Used",
    "text": "12 Functions Used\n\ndplyr: select, rename, filter, mutate, count, arrange\ntidyr: pivot_longer\nggplot2: geom_bar, geom_histogram, geom_line, facet_wrap"
  },
  {
    "objectID": "example_analysis.html#checklist",
    "href": "example_analysis.html#checklist",
    "title": "Example Analysis: Breast Cancer Data",
    "section": "13 Checklist",
    "text": "13 Checklist\n\nState the Question: Describe the main question being addressed in the analysis.\nAudience: Identify the intended audience for this analysis.\nData Source: Link to the source of the data and provide a brief description of its origin.\nData Dictionary: Include a link to or display the data dictionary on the webpage.\nData Wrangling: Use at least five unique functions from the dplyr or tidyr package for data wrangling.\nVisualization: Include at least three plots, each with different geom_*() functions from ggplot2 (or equivalent).\n\nPlot Titles and Labels: Ensure all plots have titles, subtitles, captions, and axis labels that are clear and understandable.\nFaceting: Use facet_grid() or facet_wrap() in at least one plot for segmented views.\n\nExternal Image or Table: Include at least one image or table sourced from the web or locally saved (not self-created).\nCallout Blocks: Include at least two distinct callout blocks to emphasize important points.\nReferences: Use a .bib file with at least three unique citations (e.g., data sources, methods used).\nMargin Content: Add at least one piece of margin content to enhance the analysis.\nSummary: Conclude with a 4-6 sentence paragraph summarizing the analysis results.\nFunction List: At the end, list each function used from dplyr, tidyr, and ggplot2 to help verify that all requirements are met."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently an ScM student in Biostatistics at Johns Hopkins Bloomberg School of Public Health, following my completion of a first-class honours degree in Data Science at the University of Sydney. My research interests span biostatistics, spatial genomics, and machine learning, with a focus on analyzing and deriving insights from complex data.\n\n\n\nMaster of Science in Biostatistics (ScM)\nJohns Hopkins Bloomberg School of Public Health\n2024 – Present\nBachelor of Science (Honours) in Data Science\nUniversity of Sydney\n2021 – 2024\nGraduated with First Class Honours\nAwards: Vice Chancellor’s Global Mobility Scholarship, Dalyell Scholar, Charles Perkins Centre Summer Research Scholarship\nSemester Exchange\nNational University of Singapore\nAug 2022 – Dec 2022\nSummer & Winter Schools\nShanghai Jiao Tong University\n2022 & 2023\n\n\n\n\n\nSpatialFeatures Project: Developed the “SpatialFeatures” algorithm to analyze spatial genomics data at cellular and sub-cellular levels, creating a versatile R package for use across genomics datasets.\nData Analyst Intern\nDiAct Technology Co., Ltd. (Ipsos), Shanghai\nAnalyzed social media data for automotive trends, improving data processing by 30% through Python optimizations.\nUser Growth Intern\nJD.com, Beijing\nEnhanced SQL processes and developed metrics for improved user engagement recommendations.\n\n\n\n\n\nProgramming: R, Python, SQL, SAS, Java, Tableau\nLanguages: Chinese (Native), English (Fluent)\n\n\n\n\nOutside of academics, I love fitness and playing Apex Legends."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "Master of Science in Biostatistics (ScM)\nJohns Hopkins Bloomberg School of Public Health\n2024 – Present\nBachelor of Science (Honours) in Data Science\nUniversity of Sydney\n2021 – 2024\nGraduated with First Class Honours\nAwards: Vice Chancellor’s Global Mobility Scholarship, Dalyell Scholar, Charles Perkins Centre Summer Research Scholarship\nSemester Exchange\nNational University of Singapore\nAug 2022 – Dec 2022\nSummer & Winter Schools\nShanghai Jiao Tong University\n2022 & 2023"
  },
  {
    "objectID": "about.html#research-and-professional-experience",
    "href": "about.html#research-and-professional-experience",
    "title": "About",
    "section": "",
    "text": "SpatialFeatures Project: Developed the “SpatialFeatures” algorithm to analyze spatial genomics data at cellular and sub-cellular levels, creating a versatile R package for use across genomics datasets.\nData Analyst Intern\nDiAct Technology Co., Ltd. (Ipsos), Shanghai\nAnalyzed social media data for automotive trends, improving data processing by 30% through Python optimizations.\nUser Growth Intern\nJD.com, Beijing\nEnhanced SQL processes and developed metrics for improved user engagement recommendations."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "",
    "text": "Programming: R, Python, SQL, SAS, Java, Tableau\nLanguages: Chinese (Native), English (Fluent)"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About",
    "section": "",
    "text": "Outside of academics, I love fitness and playing Apex Legends."
  }
]