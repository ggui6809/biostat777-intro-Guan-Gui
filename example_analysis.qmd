---
title: "Example Analysis: Breast Cancer Data"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    number-sections: true
    fig-cap: true
    df-print: paged
    code-fold: true
    code-summary: "Show code"
    highlight-style: tango
bibliography: "references.bib"
---

```{r}
#| echo: false
#| include: false
#| warning: false
# Setup chunk for loading libraries
library(renv)
library(tidyverse)
library(skimr)
library(corrplot)
library(ggplot2)
library(caret)
library(pROC)
library(DataExplorer)
library(readr)
library(caTools) 
library(randomForest)
```

## Introduction

This analysis investigates the Breast Cancer Wisconsin (Diagnostic) dataset to determine which features are most effective in distinguishing malignant from benign tumors. Specifically, we aim to answer the question: **What variables are most influential in predicting breast cancer diagnosis?**  
This analysis is intended for **medical researchers and clinicians** interested in exploring diagnostic features that can assist in early detection and classification of breast cancer tumors. The dataset was originally collected by Dr. William H. Wolberg and is available from the [UCI Machine Learning Repository](https://doi.org/10.24432/C5DW2B) [@wolberg_breast_1993].  
A complete data dictionary is provided below.

::: {.callout-note}
This analysis provides insights into diagnostic features in breast cancer, potentially guiding more advanced machine learning models.
:::

## Data Dictionary

| Variable                 | Description                                                                            |
|--------------------------|----------------------------------------------------------------------------------------|
| `id`                     | Unique identifier for each patient                                                     |
| `Diagnosis`              | Diagnosis of the tumor (M = malignant, B = benign)                                     |
| `radius_mean`            | Mean radius: mean of distances from center to points on the perimeter                  |
| `texture_mean`           | Mean texture: standard deviation of gray-scale values                                  |
| `perimeter_mean`         | Mean perimeter: mean size of the core tumor perimeter                                  |
| `area_mean`              | Mean area: mean size of the core tumor area                                            |
| `smoothness_mean`        | Mean smoothness: local variation in radius lengths                                     |
| `compactness_mean`       | Mean compactness: calculated as (perimeter^2 / area - 1.0)                             |
| `concavity_mean`         | Mean concavity: severity of concave portions of the contour                            |
| `concave.points_mean`    | Mean concave points: number of concave portions of the contour                         |
| `symmetry_mean`          | Mean symmetry: measure of symmetry of cell nucleus                                     |
| `fractal_dimension_mean` | Mean fractal dimension: "coastline approximation" - 1                                  |
| `radius_se`              | Standard error of radius                                                               |
| `texture_se`             | Standard error of texture                                                              |
| `perimeter_se`           | Standard error of perimeter                                                            |
| `area_se`                | Standard error of area                                                                 |
| `smoothness_se`          | Standard error of smoothness                                                           |
| `compactness_se`         | Standard error of compactness                                                          |
| `concavity_se`           | Standard error of concavity                                                            |
| `concave.points_se`      | Standard error of concave points                                                       |
| `symmetry_se`            | Standard error of symmetry                                                             |
| `fractal_dimension_se`   | Standard error of fractal dimension                                                    |
| `radius_worst`           | Worst or largest value of radius (mean of the three largest values)                    |
| `texture_worst`          | Worst or largest value of texture                                                      |
| `perimeter_worst`        | Worst or largest value of perimeter                                                    |
| `area_worst`             | Worst or largest value of area                                                         |
| `smoothness_worst`       | Worst or largest value of smoothness                                                   |
| `compactness_worst`      | Worst or largest value of compactness                                                  |
| `concavity_worst`        | Worst or largest value of concavity                                                    |
| `concave.points_worst`   | Worst or largest value of concave points                                               |
| `symmetry_worst`         | Worst or largest value of symmetry                                                     |
| `fractal_dimension_worst`| Worst or largest value of fractal dimension                                            |

For a complete data dictionary, refer to the [UCI Repository's dataset description](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)).

Below is an image from the Kaggle Breast Cancer Wisconsin (Diagnostic) Data Set, representing breast cancer cells [@kaggle_breast_cancer_image].

![Breast Cancer Cells](https://storage.googleapis.com/kaggle-datasets-images/180/384/3da2510581f9d3b902307ff8d06fe327/dataset-cover.jpg)

## Data Wrangling

The data wrangling steps refine the Breast Cancer dataset for analysis.

::: {.callout-tip} 
Correlations help identify features highly associated with tumor diagnosis, aiding in feature selection for the model. 
:::

Steps:

1. Removed unnecessary columns (`id` and `X`) to focus on diagnostic features.
2. Renamed the `diagnosis` column to `Diagnosis` for clarity.
3. Dropped rows with missing values to ensure complete cases.
4. Converted the `Diagnosis` column into a factor with levels "B" (benign) and "M" (malignant).
5. Calculated correlations with the diagnosis to identify highly predictive variables, retaining only those with correlation > |0.3|.


### Functions Used

- **`select()`**: Excludes specific columns and selects highly correlated variables.
- **`rename()`**: Renames `diagnosis` to `Diagnosis` for consistency.
- **`drop_na()`**: Removes rows with missing values.
- **`mutate()`**: Creates or transforms columns, such as converting `Diagnosis` to a factor and creating a numeric version for correlation.
- **`summarize()` + `across()`**: Calculates correlation values for each feature with `Diagnosis`.
- **`pivot_longer()`**: Reshapes the data to make correlation results easier to filter.
- **`arrange()`**: Sorts correlations by their absolute values.
- **`filter()`**: Selects only variables with a high correlation to the target variable.

```{r}
#| label: descriptive-statistics
#| echo: true
#| warning: false
# Load data
data <- read.csv("example_analysis_data.csv")

# Data wrangling with dplyr and tidyr
data <- data %>%
  select(-id, -X) %>%              
  rename(Diagnosis = diagnosis) %>% 
  drop_na() %>%                     
  mutate(Diagnosis = factor(Diagnosis, levels = c("B", "M"))) 

# Calculate correlations and arrange by correlation with target variable (Diagnosis)
data_numeric <- data %>%
  mutate(Diagnosis_num = as.numeric(Diagnosis) - 1) # Converts factor levels to 0 (B) and 1 (M)

correlations <- data_numeric %>%
  select(-Diagnosis) %>% 
  summarize(across(-Diagnosis_num, ~ cor(., data_numeric$Diagnosis_num, use = "complete.obs"))) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "correlation") %>%
  arrange(desc(abs(correlation)))

# Print top 10 correlation variables
top_10_correlations <- correlations %>% head(10)
print(top_10_correlations)

cor_threshold <- 0.3
high_corr_vars <- correlations %>%
  filter(abs(correlation) >= cor_threshold) %>%
  pull(variable)

# Keep only highly correlated variables in the original data
data <- data %>%
  select(all_of(high_corr_vars), Diagnosis)

head(data)
```


## Target Variable Distribution
This bar plot shows the distribution of benign and malignant diagnoses in the dataset.

```{r}
#| label: target-variable
#| fig-height: 4
#| fig-width: 6
#| warning: false
# Plot the distribution of tumor diagnoses
ggplot(data, aes(x = Diagnosis, fill = Diagnosis)) +
  geom_bar() +
  geom_text(stat='count', aes(label=..count..), vjust=-0.3) +
  labs(
    title = "Distribution of Tumor Diagnoses in the Dataset",
    subtitle = "Benign tumors are more frequent than malignant tumors",
    caption = "This plot shows the count of benign and malignant tumors in the dataset",
    x = "Tumor Diagnosis",
    y = "Count of Cases"
  ) +
  scale_fill_manual(values = c("B" = "#00CCCC", "M" = "salmon")) +
  theme_minimal()
```

::: {.column-margin}
Benign tumors are more common than malignant tumors, providing a slightly imbalanced but sufficient dataset for training.
::: 

## Faceted Plot: Feature Distribution by Diagnosis

This faceted histogram compares the distribution of `radius_mean` between benign and malignant tumors.

```{r}
#| label: faceted-plot
#| fig-height: 6
#| fig-width: 8
#| warning: false
# Compare the distribution of 'radius_mean' across diagnoses using faceting
ggplot(data, aes(x = radius_mean, fill = Diagnosis)) +
  geom_histogram(binwidth = 0.5, alpha = 0.7, position = "identity") +
  facet_wrap(~ Diagnosis) +
  labs(
  title = "Distribution of Mean Radius by Tumor Diagnosis",
  subtitle = "Malignant tumors tend to have a larger mean radius",
  caption = "Faceted histograms showing 'radius_mean' distribution for each diagnosis",
  x = "Mean Radius",
  y = "Frequency"
  ) +
  scale_fill_manual(values = c("B" = "#00CCCC", "M" = "salmon")) +
  theme_minimal()
```

::: {.column-margin}
The faceted histograms reveal that malignant tumors generally have higher mean radius values compared to benign tumors.
::: 

## Correlation Analysis

```
#| label: correlation
#| fig-height: 10
#| fig-width: 10
#| warning: false
# Compute and visualize correlations among numeric features
correlations <- cor(data %>% select(-Diagnosis), method = "pearson")
corrplot(correlations, number.cex = .6, method = "number", type = "upper", tl.cex=1, tl.col = "black", col = COL2("RdYlBu"))
```

**Summary:** Strong correlations are seen among features like radius, perimeter, and area, indicating redundancy that can be reduced.

## Principal Component Analysis (PCA)

The dataset's dimensionality was significantly reduced using PCA, which allows the identification and removal of less relevant features without significant loss of information [@hasan2021review].

```{r}
#| label: pca-analysis
#| fig-height: 8
#| fig-width: 10
#| warning: false
# Perform PCA for dimensionality reduction
pca <- prcomp(data %>% select(-Diagnosis), scale = TRUE)

# Variance explained by each component
var_explained <- data.frame(
  Component = 1:length(pca$sdev), 
  Variance = (pca$sdev)^2, 
  Proportion = (pca$sdev)^2 / sum((pca$sdev)^2), 
  Cumulative = cumsum((pca$sdev)^2 / sum((pca$sdev)^2))
)

# Scree Plot
ggplot(var_explained[1:10,], aes(x = Component, y = Proportion)) + 
  geom_bar(stat = "identity", fill = "salmon") +
  geom_line(color = "black") +
  geom_point(color = "black") +
  labs(
    title = "Explained Variance by Principal Components",
    subtitle = "The first few components capture the majority of variance",
    caption = "Scree plot showing the variance explained by the first 10 components",
    x = "Principal Component",
    y = "Percentage of Explained Variance"
  )
```

::: {.column-margin}
PCA helped identify six components explaining 88.76% of variance.
:::

## Data Splitting

```{r}
#| label: data-split
#| echo: true
#| warning: false
# Split the data into training and testing sets
set.seed(101)
split <- sample.split(data$Diagnosis, SplitRatio = 0.8)
train <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
```

::: {.column-margin}
The training set has 455 samples, and the test set has 114 samples, with consistent diagnosis distribution.
:::

## Model Training: Random Forest

The Random Forest method works by constructing a large number of decision trees during training and outputting the most common class [@romano2023modeling]. A Random Forest model with 10-fold cross-validation is trained to predict tumor diagnoses based on the dataset features, using 1000 trees.

```{r}
#| label: model-training
#| echo: true
# Set up cross-validation
control <- trainControl(method = "cv", number = 10)

# Random Forest
rf_model <- train(Diagnosis ~ ., data = train, method = "rf", ntree = 1000, trControl = control, importance = TRUE)
```

## Model Evaluation

```{r}
#| label: model-evaluation
#| echo: true
#| warning: false
# Evaluate the Random Forest model on the test set
evaluate_model <- function(model, test_data) {
  predictions <- predict(model, newdata = test_data)
  cm <- confusionMatrix(predictions, test_data$Diagnosis)
  auc_value <- auc(roc(test_data$Diagnosis, as.numeric(predictions)))
  data.frame(
    Accuracy = cm$overall['Accuracy'],
    Sensitivity = cm$byClass['Sensitivity'],
    Specificity = cm$byClass['Specificity'],
    AUC = auc_value
  )
}

rf_results <- evaluate_model(rf_model, test)
rf_results
```

::: {.column-margin}
The model achieves high accuracy and AUC, indicating effective classification of benign and malignant cases.
::: 

## Summary
The analysis of the Breast Cancer Wisconsin (Diagnostic) dataset revealed that features such as `concave.points_worst`, `perimeter_worst`, and `concave.points_mean` exhibit strong correlations with tumor diagnosis. Dimensionality reduction through PCA indicated that a few principal components account for the majority of variance, efficiently reducing feature redundancy. A Random Forest model, trained and evaluated on the dataset, achieved high accuracy and AUC, demonstrating its effectiveness in tumor classification. These findings provide essential insights into diagnostic markers, supporting potential advancements in early cancer detection and classification models.

## Functions Used

- **dplyr**: `select`, `rename`, `filter`, `mutate`, `count`, `arrange`
- **tidyr**: `pivot_longer`
- **ggplot2**: `geom_bar`, `geom_histogram`, `geom_line`, `facet_wrap`

## Checklist

- [x] **State the Question**: Describe the main question being addressed in the analysis.
- [x] **Audience**: Identify the intended audience for this analysis.
- [x] **Data Source**: Link to the source of the data and provide a brief description of its origin.
- [x] **Data Dictionary**: Include a link to or display the data dictionary on the webpage.
- [x] **Data Wrangling**: Use at least five unique functions from the `dplyr` or `tidyr` package for data wrangling.
- [x] **Visualization**: Include at least three plots, each with different `geom_*()` functions from `ggplot2` (or equivalent).
    - [x] **Plot Titles and Labels**: Ensure all plots have titles, subtitles, captions, and axis labels that are clear and understandable.
    - [x] **Faceting**: Use `facet_grid()` or `facet_wrap()` in at least one plot for segmented views.
- [x] **External Image or Table**: Include at least one image or table sourced from the web or locally saved (not self-created).
- [x] **Callout Blocks**: Include at least two distinct callout blocks to emphasize important points.
- [x] **References**: Use a `.bib` file with at least three unique citations (e.g., data sources, methods used).
- [x] **Margin Content**: Add at least one piece of margin content to enhance the analysis.
- [x] **Summary**: Conclude with a 4-6 sentence paragraph summarizing the analysis results.
- [x] **Function List**: At the end, list each function used from `dplyr`, `tidyr`, and `ggplot2` to help verify that all requirements are met.
